---
title: "Basetablecreation"
output: html_notebook
---


```{r}
### Basetable creation and splitting ###

set.seed(123)
rm(list = ls())
library(quantmod)

###### Read Data ######
load("/Users/wouterdewitte/Documents/GitHub/SMWA_Performance/data/All_tweets_all_features.RData")
```

```{r}
# Financial data
load("/Users/wouterdewitte/Documents/GitHub/SMWA_Performance/data/Financial_Data.RData")
```


```{r}
names(all_tweets)[2] <- "sentiment_by_dictionary"
tweets_agg <- aggregate(cbind(sentiment_by_basic,sentiment_weighted_mixed,sentiment_avg_mean, tweets_train$sentiment_by_dictionary,Topic_1,Topic_2) ~ all_tweets$date, all_tweets, mean)
colnames(tweets_agg) <- colnames(all_tweets)

plot(tweets_agg$date, tweets_agg$sentiment_by_dictionary,type = "o", col = "red", xlab = "Date", ylab = "Average Sentiment per Hour", main = "Sentiment", ylim = c(-2,2.1))

legend("bottomleft", legend=c("Dictionary", "Avg_Mean", "Basic","Weighted_mixed"), col=c("red", "blue", "green", "orange"), lty=1, cex=0.8)

lines(tweets_agg$date, tweets_agg$sentiment_avg_mean, type = "o", col = "blue")

lines(tweets_agg$date, tweets_agg$sentiment_by_basic, type = "o", col = "green")

lines(tweets_agg$date, tweets_agg$sentiment_weighted_mixed, type = "o", col = "orange")

```
```{r}
tweets_sum <- aggregate(cbind(Topic_1,Topic_2) ~ all_tweets$date, all_tweets, sum)
colnames(tweets_sum)[1] <- "date"

plot(tweets_sum$date, tweets_sum$Topic_1,type = "o", col = "red", xlab = "Date", ylab = "average topics occurence", main = "Sentiment", ylim = c(0,20000))

legend("topright", legend=c("Topic1: coffee experiences", "Topic2: Ukraine conflict"), col=c("red", "blue"), lty=1, cex=0.8)

lines(tweets_sum$date, tweets_sum$Topic_2, type = "o", col = "blue")


```

```{r}
#plot only where topic 2 is mentioned 

plot(tweets_agg$date, tweets_agg$Topic_1,type = "o", col = "red", xlab = "Date", ylab = "average topics occurence", main = "Sentiment", ylim = c(0,2))

legend("topright", legend=c("Topic1: coffee experiences", "Topic2: Ukraine conflict"), col=c("red", "blue"), lty=1, cex=0.8)

lines(tweets_agg$date, tweets_agg$Topic_2, type = "o", col = "blue")



```

```{r}
# Clean tweet data
all_tweets <- subset(all_tweets, select = -c(text, user_id))
names(all_tweets)[2] <- "sentiment_by_dictionary"

# Splitting in train and test set
test_dates <- sample(unique(all_tweets$date), length(unique(all_tweets$date))*0.2)
dates <- unique(all_tweets$date)
train_dates <- dates[!(dates %in% test_dates)]

tweets_test <- all_tweets[all_tweets$date %in% test_dates,]
tweets_train <- all_tweets[all_tweets$date %in% train_dates,]

# Scale 
num.cols <- sapply(all_tweets, is.numeric)

# apply on training set
mean_train <- colMeans(tweets_train[, num.cols])
sd_train <- sapply(tweets_train[, num.cols], sd)
#tweets_train[, num.cols] <- scale(tweets_train[, num.cols], center = mean_train, scale = sd_train)
tweets_train[, num.cols] <- scale(tweets_train[, num.cols], center = TRUE, scale = TRUE)

# apply on test set
tweets_test[, num.cols] <- scale(tweets_test[, num.cols], center = mean_train, scale = sd_train)


```





```{r}
# aggregate
tweets_train_agg <- aggregate(cbind(sentiment_by_basic,sentiment_weighted_mixed,sentiment_avg_mean, tweets_train$sentiment_by_dictionary,Topic_1,Topic_2) ~ tweets_train$date, tweets_train, mean)

tweets_test_agg <- aggregate(cbind(sentiment_by_basic,sentiment_weighted_mixed,sentiment_avg_mean,sentiment_by_dictionary,Topic_1,Topic_2) ~ tweets_test$date, tweets_test, mean)

# change colnames
colnames(tweets_train_agg) <- colnames(tweets_train)
colnames(tweets_test_agg) <- colnames(tweets_test)
```



```{r}
# Making basetable
basetable_train <- merge(tweets_train_agg,financial_data,by="date")
basetable_test <- merge(tweets_test_agg,financial_data,by="date")
```

```{r}
#impute the missing values  
print(sum(is.na(basetable_train))) #no missing values
print(sum(is.na(basetable_test))) #no missing values


```






```{r}
#scaling the financial data 
names(basetable_train[,15:20])

# MinMaxScale sentiments

# apply on training set
mean_train <- colMeans(basetable_train[, 15:20])
sd_train <- sapply(basetable_train[, 15:20], sd)
#tweets_train[, num.cols] <- scale(tweets_train[, num.cols], center = mean_train, scale = sd_train)
basetable_train[, 15:20] <- scale(basetable_train[, 15:20], center = TRUE, scale = TRUE)

# apply on test set
basetable_test[, 15:20] <- scale(basetable_test[, 15:20], center = mean_train, scale = sd_train)


```

```{r}


```



```{r}
# choosing a sentiment column
basetable_train <- subset(basetable_train, select = -c(sentiment_by_basic,sentiment_weighted_mixed,sentiment_avg_mean))

basetable_test <- subset(basetable_test, select = -c(sentiment_by_basic,sentiment_weighted_mixed,sentiment_avg_mean))
```

```{r}
# missing values
colMeans(is.na(basetable_train))
colMeans(is.na(basetable_test))

# Handle outliers
# handle_outlier_z <- function(col){
#   col_z <- scale(col)
#   ifelse(abs(col_z)>3,
#          sign(col_z)*3*attr(col_z,"scaled:scale") + attr(col_z,"scaled:center"), col)
# }
# basetable_train[, num.cols] <-  sapply(basetable_train[, num.cols], FUN = handle_outlier_z)
```

```{r}


```

```{r}
save(basetable_train, file = "/Users/wouterdewitte/Documents/GitHub/SMWA_Performance/data/basetable_train.RData")
save(basetable_test, file = "/Users/wouterdewitte/Documents/GitHub/SMWA_Performance/data/basetable_test.RData")
```




