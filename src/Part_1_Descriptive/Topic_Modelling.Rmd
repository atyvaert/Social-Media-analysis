---
title: "latent Dirichlet allocation"
output: html_notebook
---
 Applying Latent Dirichlet allocation in order to find the most important topic groups in Netflix Tweets 


```{r}
if (!require("pacman")) install.packages("pacman") ; require("pacman")
p_load(tidyverse, topicdoc, topicmodels, tm, textcat, rlang, tidytext, ggmap)
p_load(tidyverse, rtweet, httpuv, stringr, qdap, httr, wordcloud2, tm, tidytext, wordcloud)



all_tweets %>% glimpse() 
#only use the text from the tweets 
text <- all_tweets$text

```



```{r}
#develop model on smaller data set 

set.seed(123)
sample <- sample(unique(text), 
                 round(length(unique(text))/10), 
                 replace = FALSE)
length(sample)
```

Make sure that the sample is not too small 

```{r}
#applying pre processing to the smaller sample 
myCorpus <- Corpus(VectorSource(sample)) %>%
  tm_map(content_transformer(str_to_lower)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(stripWhitespace) 
  
myStopwords <- c(stopwords('english'),"the", "and",)
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)

```
```{r}
dtm1 <- DocumentTermMatrix(myCorpus)
inspect(dtm1)
    
```





```{r}
#chose sprasety 

final_dtm <- removeSparseTerms(dtm1, 0.95) 
final_dtm

```
```{r}
#remove rows which have only 0 elements: each should contain at least 1 non zero 
rowTotals <- apply(final_dtm , 1, sum) #Find the sum of words in each Document
dtm_new   <- final_dtm[rowTotals> 0, ]
dtm_new
```


```{r}
#not only calculate the UMass but also try to find different likelihood measures 
#AIC and BIC 
topics <- seq(2,10,1)
alpha <- seq(0.2,0.8,0.2)
beta <- seq(0.2,0.8,0.2)
grid <- expand.grid(topics, alpha, beta)
colnames(grid) <- c('topics', 'alpha', 'beta')
grid$UMass <- rep(-99999999999, nrow(grid))
grid$LL <- rep(-99999999999, nrow(grid))

#This takes a while ==> best to load fitted grid
for (i in 1:nrow(grid)){
  K <- grid[i,'topics']
  a <- grid[i,'alpha']
  b <- grid[i,'beta']
  lda <- LDA(dtm_new, control = list(alpha = a, estimate.beta = b), k = K)
  grid[i,'UMass'] <- mean(topic_coherence(lda, dtm_new, top_n_tokens = 30))
}


```



```{r}

optimal_settings <- grid %>% filter(UMass == max(UMass))
optimal_settings

```


```{r}
grid %>% 
  filter(alpha==0.4 & beta==0.2) %>% 
  ggplot(aes(x = topics, y = UMass)) + geom_line() 
```


```{r}
grid %>% 
  filter(alpha==0.2 & beta==0.2) %>% 
  ggplot(aes(x = topics, y = UMass)) + geom_line() 
```


```{r}
lda <- LDA(dtm_new, 
           control = list(alpha = optimal_settings$alpha, 
                          estimate.beta = optimal_settings$beta),
           k = 8)

terms(lda, 20)


```

```{r}
lda <- LDA(dtm_new, 
           control = list(alpha = optimal_settings$alpha, 
                          estimate.beta = optimal_settings$beta),
           k = 2)

terms(lda, 20)
```

Now we will deploy topic modelling on the full data set 

```{r}
set.seed(123)
sample <- sample(unique(text), 
                 round(length(unique(text))), 
                 replace = FALSE)

myCorpus <- Corpus(VectorSource(sample)) %>%
  tm_map(content_transformer(str_to_lower)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(stripWhitespace) 
  
myStopwords <- c(stopwords('english'), "the", "and")
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)

dtm1 <- DocumentTermMatrix(myCorpus)
inspect(dtm1)

#chose sprasety 

final_dtm <- removeSparseTerms(dtm1, 0.95) 
final_dtm

rowTotals <- apply(final_dtm , 1, sum) #Find the sum of words in each Document
dtm_new   <- final_dtm[rowTotals> 0, ]
dtm_new

topics <- seq(2,10,1)
alpha <- seq(0.2,0.8,0.2)
beta <- seq(0.2,0.8,0.2)
grid <- expand.grid(topics, alpha, beta)
colnames(grid) <- c('topics', 'alpha', 'beta')
grid$UMass <- rep(-99999999999, nrow(grid))
grid$LL <- rep(-99999999999, nrow(grid))

#This takes a while ==> best to load fitted grid
for (i in 1:nrow(grid)){
  K <- grid[i,'topics']
  a <- grid[i,'alpha']
  b <- grid[i,'beta']
  lda <- LDA(dtm_new, control = list(alpha = a, estimate.beta = b), k = K)
  grid[i,'UMass'] <- mean(topic_coherence(lda, dtm_new, top_n_tokens = 30))
}


```



```{r}

optimal_settings <- grid %>% filter(UMass == max(UMass))
optimal_settings

```
Lowest UMass for 6 topics but this is too much sparse variables in our final model, for this reason we choose 2

```{r}
grid %>% 
  filter(alpha==0.4 & beta==0.4) %>% 
  ggplot(aes(x = topics, y = UMass)) + geom_line() 

```



```{r}
grid %>% 
  filter(alpha==0.2 & beta==0.2) %>% 
  ggplot(aes(x = topics, y = UMass)) + geom_line() 
```


```{r}
lda <- LDA(dtm_new, 
           control = list(alpha = 0.4, 
                          estimate.beta = 0.4),
           k = 2)

terms(lda, 20)
```


```{r}
Topics_Frame <- head(as.data.frame(terms(lda, 20)),5)
Topics_Frame
```

```{r}
setwd("/Users/konstantinlazarov/Documents/GitHub/SMWA_Performance/data")
save(Topics_Frame, file = "Topics_Frame.RData")
```


```{r}
#make a variable for each column that indicates if the variable was mentioned 
library(tm)
text <- all_tweets$text
Topic1 <- Topics_Frame$`Topic 1`
Topic2 <- Topics_Frame$`Topic 2`


Topic1_count <- numeric(length(text))
Topic2_count <- numeric(length(text))

for(idx in 0:length(text)){
  split <-removePunctuation(toString(text[idx]))
  clean  <- strsplit(str_to_lower(split, locale = "en"), split = " ")
  Topic1_count[idx] <- sum(Topic1 %in% clean[[1]])
  Topic2_count[idx] <- sum(Topic2 %in% clean[[1]])
}

#adding these elements to the base table 
all_tweets["Topic1"] <- Topic1_count
all_tweets["Topic2"] <- Topic2_count
all_tweets

#save the new dataset 
setwd("/Users/konstantinlazarov/Documents/GitHub/SMWA_Performance/data")
save(all_tweets, file = "All_tweets_with_Topics.RData")

```




