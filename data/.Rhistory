b <- colSums(e)
rand <- data.frame()
for (i in 1: length(a)){
for (j in 1:length(b)){
rand[i,j] <- a[i]*b[j]
}
}
# bad measure of cluster quality
(trace_e <- sum(diag(as.matrix(e))))
# good measure of cluster quality
(Q <- sum(diag(as.matrix(e))) - sum(diag(as.matrix(rand))))
# create the data and the network
names <- c('A','B','C','D','E','F','G','H','I','J')
master <- c(rep('DA',6), rep('OR',4))
Students <- data.frame(name=names, master=master)
StudentsNetwork <- data.frame(from=c('A','A','A','A','B','B','C','C','D','D','D', 'E','F','F','G','G','H','H','I'),
to=c('B','C','D','E','C','D','D','G','E','F', 'G','F','G','I','I','H','I','J','J'),
label=c(rep('dd',7),'do','dd','dd','do','dd','do','do',rep('oo',5)))
names <- c('Tom', 'Arno', 'Sofie', 'Jan', 'Karen', 'Laura')
master <- c(rep('B', 3), rep('E',3))
names <- c("Tom", "Arno", "Sofie", "Jan", "Karen", "Laura")
master <- c(rep('B', 3), rep('E',3))
Students <- data.frame(name = names, master = masters)
names <- c("Tom", "Arno", "Sofie", "Jan", "Karen", "Laura")
masters <- c(rep('B', 3), rep('E',3))
Students <- data.frame(name = names, master = masters)
StudentsNetwork <- data.frame(from = c("Tom", "Sofie", "Karen"),
to = c("Arno", "Jan", "Laura"),
label = c("bb", "be", "ee"))
rm(masters)
master <- c(rep('B', 3), rep('E',3))
g <- graph_from_data_frame(StudentsNetwork, directed = FALSE)
V(g)$master <- Students$master
layout <- layout_nicely(g)
plot.igraph(g,
edge.label = NA,
edge.color = ifelse(StudentsNetwork$label == "bb", "blue",
ifelse(StudentsNetwork$label == "ee", "darkgreen", "red")))
plot.igraph(g,
edge.label = NA,
edge.color = ifelse(StudentsNetwork$label == "bb", "blue",
ifelse(StudentsNetwork$label == "ee", "darkgreen", "red")),
layout = layout,
vertex.label = V(g)$name,
vertex.color = ifelse(V(g)$master == 'B', "blue", "darkgreen"),
vertex.label.color = "white")
g <- graph_from_data_frame(StudentsNetwork, directed = FALSE)
V(g)$master <- Students$master
layout <- layout_nicely(g)
plot.igraph(g,
edge.label = NA,
edge.color = ifelse(StudentsNetwork$label == "bb", "blue",
ifelse(StudentsNetwork$label == "ee", "darkgreen", "red")),
layout = layout,
vertex.label = V(g)$name,
vertex.color = ifelse(V(g)$master == 'B', "blue", "darkgreen"),
vertex.label.color = "white")
library(igraph)
plot.igraph(g,
edge.label = NA,
edge.color = ifelse(StudentsNetwork$label == "bb", "blue",
ifelse(StudentsNetwork$label == "ee", "darkgreen", "red")),
layout = layout,
vertex.label = V(g)$name,
vertex.color = ifelse(V(g)$master == 'B', "blue", "darkgreen"),
vertex.label.color = "white")
names <- c("Tom", "Arno", "Sofie", "Jan", "Karen", "Laura")
master <- c(rep('B', 3), rep('E',3))
Students <- data.frame(name = names, master = masters)
Students <- data.frame(name = names, master = master)
StudentsNetwork <- data.frame(from = c("Tom", "Sofie", "Karen"),
to = c("Arno", "Jan", "Laura"),
label = c("bb", "be", "ee"))
g <- graph_from_data_frame(StudentsNetwork, directed = FALSE)
V(g)$master <- Students$master
layout <- layout_nicely(g)
plot.igraph(g,
edge.label = NA,
edge.color = ifelse(StudentsNetwork$label == "bb", "blue",
ifelse(StudentsNetwork$label == "ee", "darkgreen", "red")),
layout = layout,
vertex.label = V(g)$name,
vertex.color = ifelse(V(g)$master == 'B', "blue", "darkgreen"),
vertex.label.color = "white")
dev.off()
plot.igraph(g,
edge.label = NA,
edge.color = ifelse(StudentsNetwork$label == "bb", "blue",
ifelse(StudentsNetwork$label == "ee", "darkgreen", "red")),
layout = layout,
vertex.label = V(g)$name,
vertex.color = ifelse(V(g)$master == 'B', "blue", "darkgreen"),
vertex.label.color = "white")
plot.igraph(g,
edge.label = NA,
edge.color = ifelse(StudentsNetwork$label == "bb", "blue",
ifelse(StudentsNetwork$label == "ee", "darkgreen", "red")),
layout = layout,
vertex.label = V(g)$name,
vertex.color = ifelse(V(g)$master == 'B', "blue", "darkgreen"),
vertex.label.color = "white")
n_b <- length(which(V(g)$master == 'B'))
m_be <- length(which(E(g)$label == 'be'))
N <- length(V(g))
M <- length(E(g))
p <- 2*M/(N*(N-1))
n_e <- length(which(V(g)$master == 'E'))
m_ee <- length(which(E(g)$label == 'ee'))
m_bb <- length(which(E(g)$label == 'bb'))
N <- length(V(g))
M <- length(E(g))
p <- 2*M/(N*(N-1))
# calculate heterophilicity
(H <- m_be / bar_m_be)
# expected number of cross label edges in a random net
bar_m_be <- n_b*n_e*p
# calculate heterophilicity
(H <- m_be / bar_m_be)
bar_m_bb <- n_b * (n_b - 1) * p
(D_bb <- m_bb / bar_m_bb)
bar_m_ee <- n_e * (n_e - 1) * p
(D_ee <- m_ee / bar_m_ee)
bar_m_bb <- n_b * (n_b - 1)/2 * p
(D_bb <- m_bb / bar_m_bb)
bar_m_ee <- n_e * (n_e - 1)/2 * p
(D_ee <- m_ee / bar_m_ee)
# load node2vec
p_load(node2vec)
# load node2vec
p_load(node2vec)
# p_load() is from the pacman package
library(pacman)
library(pacman)
# load node2vec
p_load(node2vec)
# 1. Create the Data and the Network
names <- c('A','B','C','D','E','F','G','H','I','J')
master <- c(rep('DA',6), rep('OR',4))
Students <- data.frame(name=names, master=master)
StudentsNetwork <- data.frame(from=c('A','A','A','A','B','B','C','C','D','D','D', 'E','F','F','G','G','H','H','I'),
to=c('B','C','D','E','C','D','D','G','E','F', 'G','F','G','I','I','H','I','J','J'),
label=c(rep('dd',7),'do','dd','dd','do','dd','do','do',rep('oo',5)))
# 2. Plot the Network
g <- graph_from_data_frame(StudentsNetwork, directed = FALSE)
library(statnet)
# 2. Plot the Network
g <- graph_from_data_frame(StudentsNetwork, directed = FALSE)
library(igraph)
# 2. Plot the Network
g <- graph_from_data_frame(StudentsNetwork, directed = FALSE)
V(g)$master <- as.character(Students$master)
layout <- layout_nicely(g)
plot.igraph(g,
edge.label = NA,
edge.color = ifelse(StudentsNetwork$label == "dd", "blue", ifelse(StudentsNetwork$label == "do", "red", "darkgreen")),
layout = layout,
vertex.label = V(g)$name,
vertex.color = ifelse(V(g)$master == "DA", "blue", "darkgreen"),
vertex.label.color = 'white')
# Calculate number of nodes
N <- length(V(g))
n_da <- length(which(V(g)$master=='DA'))
n_or <- length(which(V(g)$master=='OR'))
# Calculate number of edges
M <- length(E(g))
m_dd <- length(which(E(g)$label=='dd'))
m_oo <- length(which(E(g)$label=='oo'))
m_do <- length(which(E(g)$label=='do'))
# Calculate connectedness
p <- 2*M/(N*(N-1))        #  = graph.density(g)
# expected number of cross label edges in a random net
bar_m_do <- n_da*n_or*p
# calculate heterophilicity
H <- m_do/bar_m_do
H
# expected number of cross label edges in a random net
bar_m_be <- n_b*n_e*p
# calculate heterophilicity
(H <- m_be / bar_m_be)
# for DA
bar_m_dd <- n_da*(n_da-1)/2*p
D_dd <- m_dd/bar_m_dd
D_dd
# for OR
bar_m_oo <- n_or*(n_or-1)/2*p
D_oo <- m_oo/bar_m_oo
D_oo
# calculate node2vec using return parameter p = 1, and in-out parameter q = 1
emb <- node2vecR(StudentsNetwork[,-3], p=1,q=1,num_walks=5,walk_length=5,dim=5)
# inspect corresponding embeddings
str(emb)
# add rowname to embedding matrix to know which node we are talking about
emb_name <- data.frame(node = row.names(emb), emb)
row.names(emb)
# calculate the cosine similarity between node G and all other nodes
cos_sim <- sim2(x = as.matrix(emb), y = as.matrix(emb[emb_name$node == "G",, drop = FALSE]), method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 10)
# statnet is a network package
library(statnet)
# p_load() is from the pacman package
library(pacman)
# load igraph package
library(igraph)
# calculate the cosine similarity between node G and all other nodes
cos_sim <- sim2(x = as.matrix(emb), y = as.matrix(emb[emb_name$node == "G",, drop = FALSE]), method = "cosine", norm = "l2")
library(text2vec)
install.packages(text2vec)
install.packages("text2vec")
library(text2vec)
# calculate the cosine similarity between node G and all other nodes
cos_sim <- sim2(x = as.matrix(emb), y = as.matrix(emb[emb_name$node == "G",, drop = FALSE]), method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 10)
# calculate node2vec using return parameter p = 0.5, and in-out parameter q = 2
emb <- node2vecR(StudentsNetwork[,-3], p=0.5,q=2,num_walks=5,walk_length=5,dim=5)
# inspect corresponding embeddings
str(emb)
# add rowname to embedding matrix to know which node we are talking about
emb_name <- data.frame(node = row.names(emb), emb)
# calculate the cosine similarity between node G and all other nodes
cos_sim <- sim2(x = as.matrix(emb), y = as.matrix(emb[emb_name$node == "G",, drop = FALSE]), method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 10)
p_load(httr, rtweet, tidyverse, wordcloud2, tm, tidytext)
get_token()
search.string <- "#Tesla"
tweets <- search_tweets(search.string, n = 1000,
include_rts = FALSE,
retryonratelimit = FALSE,
lang = 'en')
tweets_data(tweets)
text <- tweets_data(tweets) %>% pull(text)
p_load(tm,wordcloud, wordcloud2)
tdm <- TermDocumentMatrix(Corpus(VectorSource(text)))
m <- as.matrix(tdm)
library(pacman)
p_load(httr, rtweet, tidyverse, wordcloud2, tm, tidytext)
get_token()
search.string <- "#Tesla"
tweets <- search_tweets(search.string, n = 1000,
include_rts = FALSE,
retryonratelimit = FALSE,
lang = 'en')
tweets_data(tweets)
text <- tweets_data(tweets) %>% pull(text)
p_load(tm,wordcloud, wordcloud2)
tdm <- TermDocumentMatrix(Corpus(VectorSource(text)))
m <- as.matrix(tdm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- tibble(word = names(v),freq=v)
d <- d %>% filter(word != tolower(search.string)) %>% arrange(desc(freq))
options(warn=-1) #turn warnings off
wordcloud2(d)
options(warn=0) #turn warnings back on
# now with preprocessing
p_load(SnowballC, slam, tm)
text <- Corpus(VectorSource(text))
unique_word_count <- function(data){
content <- character(length(data))
if (any(class(data) %in% c("VCorpus", "Corpus","SimpleCorpus"))) {
for (i in 1:length(data)) content[i] <- data[[i]]$content
} else {
content <- data
}
uniquewords <- unique(unlist(map(
str_split(as.character(content)," "),
unique)))
length(uniquewords)
}
unique_word_count(text)
text_preprocessed <- Corpus(VectorSource(text)) %>%
tm_map(content_transformer(str_to_lower)) %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace)
unique_word_count(text_preprocessed)
myCorpus <- Corpus(VectorSource(text_preprocessed))
myStopwords <- c(stopwords('english'))
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
unique_word_count(text_preprocessed)
tdm <- TermDocumentMatrix(myCorpus)
m <- as.matrix(tdm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- tibble(word = names(v),freq=v)
d <- d %>% filter(word != tolower(search.string)) %>% arrange(desc(freq))
d <- d %>% filter(word != "the") %>% arrange(desc(freq))
d <- d %>% filter(word != "and") %>% arrange(desc(freq))
d <- d %>% filter(word != "with") %>% arrange(desc(freq))
d <- d %>% filter(word != "for") %>% arrange(desc(freq))
d <- d %>% filter(word != "you") %>% arrange(desc(freq))
d <- d %>% filter(word != "tesla") %>% arrange(desc(freq))
d <- d %>% filter(word != "elon") %>% arrange(desc(freq))
d <- d %>% filter(word != "musk") %>% arrange(desc(freq))
d <- d %>% filter(word != "elonmusk") %>% arrange(desc(freq))
options(warn=-1) #turn warnings off
wordcloud_tesla <- wordcloud2(d)
options(warn=0) #turn warnings back on
wordcloud_tesla <- wordcloud2(d)
# wordgraph
text <- iconv(text,'latin1', 'ascii', sub = '')
create_document_term_matrix <- function(data){
p_load(SnowballC, tm)
myCorpus <- Corpus(VectorSource(data))
cat("Transform to lower case \n")
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
cat("Remove punctuation \n")
myCorpus <- tm_map(myCorpus, removePunctuation)
cat("Remove numbers \n")
myCorpus <- tm_map(myCorpus, removeNumbers)
cat("Remove stopwords \n")
myStopwords <- c(stopwords('english'))
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
# cat("Stem corpus \n")
# myCorpus = tm_map(myCorpus, stemDocument);
# myCorpus = tm_map(myCorpus, stemCompletion, dictionary=dictCorpus);
cat("Create document by term matrix \n")
myDtm <- DocumentTermMatrix(myCorpus, control = list(wordLengths = c(2, Inf)))
myDtm
}
doc_term_mat <- create_document_term_matrix(text)
create_adjacency_matrix <- function(object, probs=0.99){
#object = output from function create_document_term_matrix (a document by term matrix)
#probs = select only vertexes with degree greater than or equal to quantile given by the value of probs
cat("Create adjacency matrix \n")
p_load(sna)
mat <- as.matrix(object)
mat[mat >= 1] <- 1 #change to boolean (adjacency) matrix
Z <- t(mat) %*% mat
cat("Apply filtering \n")
ind <- sna::degree(as.matrix(Z),cmode = "indegree") >= quantile(sna::degree(as.matrix(Z),cmode = "indegree"),probs=0.99)
#ind <- sna::betweenness(as.matrix(Z)) >= quantile(sna::betweenness(as.matrix(Z)),probs=0.99)
Z <- Z[ind,ind]
cat("Resulting adjacency matrix has ",ncol(Z)," rows and columns \n")
dim(Z)
list(Z=Z,termbydocmat=object,ind=ind)
}
adj_mat <- create_adjacency_matrix(doc_term_mat)
adj_mat[[1]][1:5,1:5]
plot_network <- function(object){
#Object: output from the create_adjacency_matrix function
#Create graph from adjacency matrix
p_load(igraph)
g <- graph.adjacency(object$Z, weighted=TRUE, mode ='undirected')
g <- simplify(g)
#Set labels and degrees of vertices
V(g)$label <- V(g)$name
V(g)$degree <- igraph::degree(g)
layout <- layout.auto(g)
opar <- par()$mar; par(mar=rep(0, 4)) #Give the graph lots of room
#Adjust the widths of the edges and add distance measure labels
#Use 1 - binary (?dist) a proportion distance of two vectors
#The binary distance (or Jaccard distance) measures the dissimilarity, so 1 is perfect and 0 is no overlap (using 1 - binary)
edge.weight <- 7  #A maximizing thickness constant
z1 <- edge.weight*(1-dist(t(object$termbydocmat)[object$ind,], method="binary"))
E(g)$width <- c(z1)[c(z1) != 0] #Remove 0s: these won't have an edge
clusters <- spinglass.community(g)
cat("Clusters found: ", length(clusters$csize),"\n")
cat("Modularity: ", clusters$modularity,"\n")
plot(g, layout=layout, vertex.color=rainbow(4)[clusters$membership], vertex.frame.color=rainbow(4)[clusters$membership] )
}
plot_network(adj_mat)
load("~/GitHub/SMWA_Performance/data/Scrape5_03Till13_03_Starbucks.RData")
tweets2
library(pacman)
p_load(twitteR)
tweets2
getwd()
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\GitHub\\SMWA_Performance\\data")
scrape1 <- load("First_Scrape9_03.Rdata")
scrape1
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\GitHub\\SMWA_Performance\\data")
load("First_Scrape9_03.Rdata")
tweets
load("Scrape_09_03.Rdata")
load("First_Scrape9_03.Rdata", "Scrape_09_03.Rdata")
lapply("First_Scrape9_03.Rdata", "Scrape_09_03.Rdata", load, .GlovalEnv)
tweets1 <- load("First_Scrape9_03")
tweets1 <- load("First_Scrape9_03.Rdata")
write.csv(tweets, "First_Scrape9_03.csv")
load("First_Scrape9_03.Rdata")
tweets1 <- load("First_Scrape9_03.Rdata")
tweets1
tweets
tweets1 <-tweets
tweets1
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\GitHub\\SMWA_Performance\\data")
load("First_Scrape9_03.Rdata")
tweets1 <-tweets
load("Scrape_09_03.Rdata")
tweets2 <- tweets
load("Scrape1_10_03.Rdata")
tweets3 <- tweets
load("Scrape1_13_03.Rdata")
tweets4 <- tweets
load("Scrape1_14_03.Rdata")
tweets5 <- tweets
load("Scrape1_15_03.Rdata")
tweets6 <- tweets
union(tweets1, tweets2)
tweets_total <- union(tweets1, tweets2)
class(tweets_total)
summary(tweets_total)
head(tweets_total)
tweets_total[1, ]
head(tweets_total, 1)
tweets_total
tweets1
colnames(tweets1)
df <- do.call("rbind", lapply(tweets1, as.data.frame))
class(tweets1)
tweets1
class(tweets2)
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\GitHub\\SMWA_Performance\\data")
load("First_Scrape9_03.Rdata")
tweets1 <-tweets
load("Scrape_09_03.Rdata")
tweets2 <- tweets
load("Scrape1_10_03.Rdata")
tweets3 <- tweets
load("Scrape1_13_03.Rdata")
tweets4 <- tweets
load("Scrape1_14_03.Rdata")
tweets5 <- tweets
load("Scrape1_15_03.Rdata")
tweets6 <- tweets
class(tweets1)
tl <- twListToDF(tweets1)
library(twitteR)
tl <- twListToDF(tweets1)
colnames(tweets1)
tweets1[, ("user_id", "text")]
tweets1$user_id
tweets[, c("user_id", "text")]
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\GitHub\\SMWA_Performance\\data")
load("First_Scrape9_03.Rdata")
tweets1 <-tweets[, c("user_id", "text")]
load("Scrape_09_03.Rdata")
tweets2 <- tweets[, c("user_id", "text")]
load("Scrape1_10_03.Rdata")
tweets3 <- tweets[, c("user_id", "text")]
load("Scrape1_13_03.Rdata")
tweets4 <- tweets[, c("user_id", "text")]
load("Scrape1_14_03.Rdata")
tweets5 <- tweets[, c("user_id", "text")]
load("Scrape1_15_03.Rdata")
tweets6 <- tweets[, c("user_id", "text")]
library(twitteR)
tl <- twListToDF(tweets1)
class(tweets1)
tweets1
View(tweets1)
View(tweets1)
class(tweets1)
tweets1 <- as.data.frame(tweets[, c("user_id", "text")])
class(tweets1)
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\GitHub\\SMWA_Performance\\data")
load("First_Scrape9_03.Rdata")
tweets1 <- as.data.frame(tweets[, c("user_id", "text")])
load("Scrape_09_03.Rdata")
tweets2 <- as.data.frame(tweets[, c("user_id", "text")])
load("Scrape1_10_03.Rdata")
tweets3 <- as.data.frame(tweets[, c("user_id", "text")])
load("Scrape1_13_03.Rdata")
tweets4 <- as.data.frame(tweets[, c("user_id", "text")])
load("Scrape1_14_03.Rdata")
tweets5 <- as.data.frame(tweets[, c("user_id", "text")])
load("Scrape1_15_03.Rdata")
tweets6 <- as.data.frame(tweets[, c("user_id", "text")])
union(tweets1, tweets2, tweets3, tweets4, tweets5, tweets6)
all_tweets <- union(tweets1, tweets2)
all_tweets <- union(all_tweets, tweets3)
all_tweets <- union(all_tweets, tweets4)
all_tweets <- union(all_tweets, tweets5)
all_tweets <- union(all_tweets, tweets6)
class(all_tweets)
all_tweets <- as.data.frame(union(tweets1, tweets2))
all_tweets <- rbind(tweets1, tweets2)
class(all_tweets)
all_tweets <- rbind(tweets1, tweets2, tweets3)
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\GitHub\\SMWA_Performance\\data")
load("First_Scrape9_03.Rdata")
tweets1 <- as.data.frame(tweets[, c("user_id", "text")])
load("Scrape_09_03.Rdata")
tweets2 <- as.data.frame(tweets[, c("user_id", "text")])
load("Scrape1_10_03.Rdata")
tweets3 <- as.data.frame(tweets[, c("user_id", "text")])
load("Scrape1_13_03.Rdata")
tweets4 <- as.data.frame(tweets[, c("user_id", "text")])
load("Scrape1_14_03.Rdata")
tweets5 <- as.data.frame(tweets[, c("user_id", "text")])
load("Scrape1_15_03.Rdata")
tweets6 <- as.data.frame(tweets[, c("user_id", "text")])
all_tweets <- rbind(tweets1, tweets2, tweets3, tweets4, tweets5, tweets6)
class(all_tweets)
colnames(tweets)
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\GitHub\\SMWA_Performance\\data")
load("First_Scrape9_03.Rdata")
tweets1 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape_09_03.Rdata")
tweets2 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_10_03.Rdata")
tweets3 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_13_03.Rdata")
tweets4 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_14_03.Rdata")
tweets5 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_15_03.Rdata")
tweets6 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
all_tweets <- rbind(tweets1, tweets2, tweets3, tweets4, tweets5, tweets6)
colnames(all_tweets) <- c("user_id", "date", "text")
class(all_tweets)
View(all_tweets)
save(all_tweets, file = "all_tweets.Rdata")
View(all_tweets)
