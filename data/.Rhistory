#Or tidy
#see ?tidy.LDA for more info
tidy(lda, matrix = 'gamma')
terms(lda, 20)
#Make a tibble ready for visualization
#We want one row per listing together with the top topic and its distribution
visualisation_set <- sample %>%
group_by(listing_id) %>%
summarise_at(c('topic1','topic2', "topic3", "topic4"), mean, na.rm = TRUE) %>%
pivot_longer(c('topic1','topic2',, "topic3", "topic4"), names_to = "topic", values_to = "cnt") %>%
group_by(listing_id) %>%
slice_max(cnt)
sample[,c('topic1','topic2', "topic3", "topic4")] <- lda@gamma
lda@gamma
#sample <- sample[rowTotals > 0,]
sample[,c('topic1','topic2', "topic3", "topic4")] <- lda@gamma
sample <- sample[rowTotals > 0,]
sample[,c('topic1','topic2', "topic3", "topic4")] <- lda@gamma
#Make a tibble ready for visualization
#We want one row per listing together with the top topic and its distribution
visualisation_set <- sample %>%
group_by(listing_id) %>%
summarise_at(c('topic1','topic2', "topic3", "topic4"), mean, na.rm = TRUE) %>%
pivot_longer(c('topic1','topic2',, "topic3", "topic4"), names_to = "topic", values_to = "cnt") %>%
group_by(listing_id) %>%
slice_max(cnt)
#Import spatial information
coordinates <- read_csv('listings.csv')
merged_set <- coordinates %>%
mutate(listing_id = id) %>%
inner_join(visualisation_set, 'listing_id')
get_stamenmap(bbox = c(left = -122.7276, bottom = 37.6373, right = -121.9688, top = 37.8881),
zoom = 10, maptype = "toner-lite") %>% ggmap()
qmplot(longitude, latitude,
data = merged_set,
maptype = "toner-lite",
size = cnt, color = topic)
########################################
## DESCPRITIVE ##
rm(list=ls()) #Clean the environment
#install.packages("twitteR")
library(twitteR)
search.string <- "#Starbucks"
help("searchTwitter")
#install.packages("twitteR")
library(twitteR)
consumer_key <- 'u7KJn5vc0eAhnEIAdxuEftRu6'
consumer_secret <- 'col5xyDsno07FrlY88M6tyHYTt5uEggJwN9A4AbQmVHO75ZfLU'
access_token <- '1492166932132474880-2bedVAngfyyRLo7Wldp5Kact5zHHHY'
access_secret <- 'ICzs9yOImM5PeKnlJnn5SZ9Q9fVFIRsftm35y0pIXIA7v'
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
#install.packages("twitteR")
library(twitteR)
consumer_key <- 'u7KJn5vc0eAhnEIAdxuEftRu6'
consumer_secret <- 'col5xyDsno07FrlY88M6tyHYTt5uEggJwN9A4AbQmVHO75ZfLU'
access_token <- '1492166932132474880-2bedVAngfyyRLo7Wldp5Kact5zHHHY'
access_secret <- 'ICzs9yOImM5PeKnlJnn5SZ9Q9fVFIRsftm35y0pIXIA7v'
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
#install.packages("twitteR")
library(twitteR)
consumer_key <- 'u7KJn5vc0eAhnEIAdxuEftRu6'
consumer_secret <- 'col5xyDsno07FrlY88M6tyHYTt5uEggJwN9A4AbQmVHO75ZfLU'
access_token <- '1492166932132474880-2bedVAngfyyRLo7Wldp5Kact5zHHHY'
access_secret <- 'ICzs9yOImM5PeKnlJnn5SZ9Q9fVFIRsftm35y0pIXIA7v'
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
load("/Users/konstantinlazarov/Documents/GitHub/SMWA_Performance/data/all_tweets.Rdata")
if (!require("pacman")) install.packages("pacman") ; require("pacman")
p_load(tidyverse, topicdoc, topicmodels, tm, textcat, rlang, tidytext, ggmap)
p_load(tidyverse, rtweet, httpuv, stringr, qdap, httr, wordcloud2, tm, tidytext, wordcloud)
if (!require("pacman")) install.packages("pacman") ; require("pacman")
p_load(tidyverse, topicdoc, topicmodels, tm, textcat, rlang, tidytext, ggmap)
p_load(tidyverse, rtweet, httpuv, stringr, qdap, httr, wordcloud2, tm, tidytext, wordcloud)
all_tweets %>% glimpse()
#only use the text from the tweets
text <- all_tweets$text
#develop model on smaller data set
set.seed(123)
sample <- sample(unique(text),
round(length(unique(text))/100),
replace = FALSE)
length(sample)
#develop model on smaller data set
set.seed(123)
sample <- sample(unique(text),
round(length(unique(text))/10),
replace = FALSE)
length(sample)
#applying pre processing to the smaller sample
myCorpus <- Corpus(VectorSource(sample)) %>%
tm_map(content_transformer(str_to_lower)) %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace)
myStopwords <- c(stopwords('english'),"starbucks", "the", "and", "📌")
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
dtm1 <- DocumentTermMatrix(myCorpus)
inspect(dtm1)
dtm1$dimnames
😍
#chose sprasety
final_dtm <- removeSparseTerms(dtm1, 0.95)
final_dtm
#remove rows which have only 0 elements: each should contain at least 1 non zero
rowTotals <- apply(final_dtm , 1, sum) #Find the sum of words in each Document
dtm_new   <- final_dtm[rowTotals> 0, ]
dtm_new
#not only calculate the UMass but also try to find different likelihood measures
#AIC and BIC
topics <- seq(2,10,1)
alpha <- seq(0.2,0.8,0.2)
beta <- seq(0.2,0.8,0.2)
grid <- expand.grid(topics, alpha, beta)
colnames(grid) <- c('topics', 'alpha', 'beta')
grid$UMass <- rep(-99999999999, nrow(grid))
grid$LL <- rep(-99999999999, nrow(grid))
#This takes a while ==> best to load fitted grid
for (i in 1:nrow(grid)){
K <- grid[i,'topics']
a <- grid[i,'alpha']
b <- grid[i,'beta']
lda <- LDA(dtm_new, control = list(alpha = a, estimate.beta = b), k = K)
grid[i,'UMass'] <- mean(topic_coherence(lda, dtm_new, top_n_tokens = 30))
}
optimal_settings <- grid %>% filter(UMass == max(UMass))
optimal_settings
grid %>%
filter(alpha==0.4 & beta==0.2) %>%
ggplot(aes(x = topics, y = UMass)) + geom_line()
grid %>%
filter(alpha==0.2 & beta==0.2) %>%
ggplot(aes(x = topics, y = UMass)) + geom_line()
lda <- LDA(dtm_new,
control = list(alpha = optimal_settings$alpha,
estimate.beta = optimal_settings$beta),
k = 8)
terms(lda, 20)
lda <- LDA(dtm_new,
control = list(alpha = optimal_settings$alpha,
estimate.beta = optimal_settings$beta),
k = 3)
terms(lda, 20)
lda <- LDA(dtm_new,
control = list(alpha = optimal_settings$alpha,
estimate.beta = optimal_settings$beta),
k = 2)
lda <- LDA(dtm_new,
control = list(alpha = optimal_settings$alpha,
estimate.beta = optimal_settings$beta),
k = 2)
terms(lda, 20)
sample <- sample(unique(text),
round(length(unique(text))),
replace = FALSE)
sample
set.seed(123)
sample <- sample(unique(text),
round(length(unique(text))),
replace = FALSE)
myCorpus <- Corpus(VectorSource(sample)) %>%
tm_map(content_transformer(str_to_lower)) %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace)
myStopwords <- c(stopwords('english'),"starbucks", "the", "and", "📌")
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
dtm1 <- DocumentTermMatrix(myCorpus)
inspect(dtm1)
#chose sprasety
final_dtm <- removeSparseTerms(dtm1, 0.95)
final_dtm
rowTotals <- apply(final_dtm , 1, sum) #Find the sum of words in each Document
dtm_new   <- final_dtm[rowTotals> 0, ]
dtm_new
topics <- seq(2,10,1)
alpha <- seq(0.2,0.8,0.2)
beta <- seq(0.2,0.8,0.2)
grid <- expand.grid(topics, alpha, beta)
colnames(grid) <- c('topics', 'alpha', 'beta')
grid$UMass <- rep(-99999999999, nrow(grid))
grid$LL <- rep(-99999999999, nrow(grid))
#This takes a while ==> best to load fitted grid
for (i in 1:nrow(grid)){
K <- grid[i,'topics']
a <- grid[i,'alpha']
b <- grid[i,'beta']
lda <- LDA(dtm_new, control = list(alpha = a, estimate.beta = b), k = K)
grid[i,'UMass'] <- mean(topic_coherence(lda, dtm_new, top_n_tokens = 30))
}
optimal_settings <- grid %>% filter(UMass == max(UMass))
optimal_settings
grid %>%
filter(alpha==0.4 & beta==0.6) %>%
ggplot(aes(x = topics, y = UMass)) + geom_line()
grid %>%
filter(alpha==0.2 & beta==0.2) %>%
ggplot(aes(x = topics, y = UMass)) + geom_line()
grid %>%
filter(alpha==0.2 & beta==0.4) %>%
ggplot(aes(x = topics, y = UMass)) + geom_line()
grid %>%
filter(alpha==0.4 & beta==0.4) %>%
ggplot(aes(x = topics, y = UMass)) + geom_line()
lda <- LDA(dtm_new,
control = list(alpha = optimal_settings$alpha,
estimate.beta = optimal_settings$beta),
k = 3)
terms(lda, 20)
lda <- LDA(dtm_new,
control = list(alpha = optimal_settings$alpha,
estimate.beta = optimal_settings$beta),
k = 2)
terms(lda, 20)
list(terms(lda, 20))
list(terms(lda, 20))[0]
list(terms(lda, 20))[1]
list(terms(lda, 20))[2]
list(terms(lda, 20))[1]
list(terms(lda, 20))[1][0]
list(terms(lda, 20))[1][1]
list(terms(lda, 20))[1][1][1]
list(terms(lda, 20))[1][1][1][1]
as.data.frame(terms(lda, 20))
as.data.frame(terms(lda, 20))[1]
as.data.frame(terms(lda, 20))[2]
as.data.frame(terms(lda, 20))
lda <- LDA(dtm_new,
control = list(alpha = optimal_settings$alpha,
estimate.beta = optimal_settings$beta),
k = 3)
lda <- LDA(dtm_new,
control = list(alpha = optimal_settings$alpha,
estimate.beta = optimal_settings$beta),
k = 3)
terms(lda, 20)
lda <- LDA(dtm_new,
control = list(alpha = optimal_settings$alpha,
estimate.beta = optimal_settings$beta),
k = 2)
terms(lda, 20)
setwd("/Users/konstantinlazarov/Documents/GitHub/SMWA_Performance/data")
save(Topics_Frame, file = "Topics_Frame.RData")
Topics_Frame <- as.data.frame(terms(lda, 20))
Topics_Frame <- as.data.frame(terms(lda, 20))
Topics_Frame
setwd("/Users/konstantinlazarov/Documents/GitHub/SMWA_Performance/data")
save(Topics_Frame, file = "Topics_Frame.RData")
grid %>%
filter(alpha==0.2 & beta==0.4) %>%
ggplot(aes(x = topics, y = UMass)) + geom_line()
grid %>%
filter(alpha==0.2 & beta==0.2) %>%
ggplot(aes(x = topics, y = UMass)) + geom_line()
grid %>%
filter(alpha==0.4 & beta==0.4) %>%
ggplot(aes(x = topics, y = UMass)) + geom_line()
grid %>%
filter(alpha==0.4 & beta==0.6) %>%
ggplot(aes(x = topics, y = UMass)) + geom_line()
grid %>%
filter(alpha==0.4 & beta==0.4) %>%
ggplot(aes(x = topics, y = UMass)) + geom_line()
lda <- LDA(dtm_new,
control = list(alpha = 0.4,
estimate.beta = 0.4),
k = 2)
terms(lda, 20)
Topics_Frame <- as.data.frame(terms(lda, 20))
Topics_Frame
Topics_Frame <- as.data.frame(terms(lda, 20))[:,1:5]
Topics_Frame <- as.data.frame(terms(lda, 20))[1:5]
Topics_Frame <- head(as.data.frame(terms(lda, 20)),5)
Topics_Frame
setwd("/Users/konstantinlazarov/Documents/GitHub/SMWA_Performance/data")
save(Topics_Frame, file = "Topics_Frame.RData")
########################################
## DESCPRITIVE ##
rm(list=ls()) #Clean the environment
setwd("/Users/konstantinlazarov/Desktop/SMWA/Group_Assignement")
# Install and load packages
if (!require("pacman")) install.packages("pacman") ; require("pacman")
p_load(tidyverse, rtweet, httpuv, stringr, qdap, httr, wordcloud2, tm, tidytext, wordcloud)
#install.packages("twitteR")
library(twitteR)
source('tokensandkeys.R')
token <- get_token()
search.string <- "#Starbucks"
tweets <- search_tweets(search.string, n = 10000,
include_rts = FALSE,
retryonratelimit = FALSE,
lang = 'en',
token = get_token())
setwd("/Users/konstantinlazarov/Documents/GitHub/SMWA_Performance/data")
save(tweets, file = "Scrape_23_03.RData")
source("~/.active-rstudio-document", echo=TRUE)
if (!require("pacman")) install.packages("pacman") ; require("pacman")
p_load(httr,rtweet,tidyverse)
rm(list = ls())
load("/Users/konstantinlazarov/Documents/GitHub/SMWA_Performance/data/all_tweets.Rdata")
all_tweets
plot(all_tweets$date)
all_tweets %>% group_by(date)
all_tweets %>% group_by(date) %>% summarise(count = count(user_id))
all_tweets %>% group_by(date) %>% summarise(aantal = count(user_id))
hist(all_tweets$date)
geom_bar(all_tweets$date)
count(all_tweets$date)
unique(all_tweets$date)
all_tweets %>% group_by(date) %>% summarise(aantal = len(unique(user_id)))
all_tweets %>% group_by(date) %>% summarise(aantal = length(unique(user_id)))
all_tweets %>% group_by(date) %>% summarise(aantal = length((user_id)))
vis <- all_tweets %>% group_by(date) %>% summarise(aantal = length((user_id)))
vis <- all_tweets %>% group_by(date) %>% summarise(aantal = length((user_id)))
vis
plot(vis$date, vis$aantal)
p_load(ggplot)
p_load(ggplot2)
ggplot(data = vis, aes(x = date, y = aantal))+geom_line()
vis$aantal
hist(vis)
ggplot(data = vis, aes(x = date, y = aantal))+geom_line()
ggplot(data = vis, aes(x = date, y = aantal))
+geom_line()
ggplot(data = vis, aes(x = date, y = aantal))
ggplot(data = vis, aes(x = date, y = vis$aantal))
ggplot(data = vis, aes(x = vis$date, y = vis$aantal))
plot(vis$date, vis$aantal)
as.factor(vis$date)
plot(as.factor(vis$date), vis$aantal)
p_load(ggplot2)
vis <- all_tweets %>% group_by(date) %>% summarise(aantal = length((user_id)))
ggplot(data = vis, aes(x = vis$date, y = vis$aantal))
plot(as.factor(vis$date), vis$aantal)
vis
load("/Users/konstantinlazarov/Documents/GitHub/SMWA_Performance/data/dictionary.RData")
#read in the dictionary.Rdata file
#rescale the valance arousal and dominance values [-4;4]
dictionary <- dictionary %>% mutate(across(where(is.numeric),function(x) x-5 ))
text <- all_tweets$text
p_load(skimr)
text <- all_tweets$text
#read in the dictionary.Rdata file
#rescale the valance arousal and dominance values [-4;4]
dictionary <- dictionary %>% mutate(across(where(is.numeric),function(x) x-5 ))
#visualize the dictionary
skim(dictionary)
p_load(skimr)
text <- all_tweets$text
#read in the dictionary.Rdata file
#rescale the valance arousal and dominance values [-4;4]
dictionary <- dictionary %>% mutate(across(where(is.numeric),function(x) x-5 ))
#visualize the dictionary
skim(dictionary)
Encoding(text) <- "latin1"
text <- iconv(text,'latin1', 'ascii', sub = '')
text
Encoding(text) <- "latin1"
text <- iconv(text,'latin1', 'ascii', sub = '')
length(text)
Encoding(text) <- "latin1"
text <- iconv(text,'latin1', 'ascii', sub = '')
#making a vector that will store each score tweet value
scoretweet <- numeric(length(text))
for (i in 1:length(text)){
#Transform text to lower case
text <- tolower(text)
#Split the tweet in words
tweetsplit <- str_split(text[i]," ")[[1]]
#Find the positions of the words in the Tweet in the dictionary
m <- match(tweetsplit, dictionary$Word)
#Which words are present in the dictionary?
present <- !is.na(m)
#Of the words that are present, select their valence
wordvalences <- dictionary$VALENCE[m[present]]
#Compute the mean valence of the tweet
scoretweet[i] <- mean(wordvalences, na.rm=TRUE)
#Handle the case when none of the words is in the dictionary
if (is.na(scoretweet[i])) scoretweet[i] <- 0 else scoretweet[i] <- scoretweet[i]
}
head(scoretweet)
plot(scoretweet)
mean(scoretweet)
hist(scoretweet)
p_load(skimr)
text <- all_tweets$text
#read in the dictionary.Rdata file
#rescale the valance arousal and dominance values [-4;4]
dictionary <- dictionary %>% mutate(across(where(is.numeric),function(x) x-5 ))
#visualize the dictionary
skim(dictionary)
load("/Users/konstantinlazarov/Documents/GitHub/SMWA_Performance/data/dictionary.RData")
p_load(skimr)
text <- all_tweets$text
#read in the dictionary.Rdata file
#rescale the valance arousal and dominance values [-4;4]
dictionary <- dictionary %>% mutate(across(where(is.numeric),function(x) x-5 ))
#visualize the dictionary
skim(dictionary)
dictionary
Encoding(text) <- "latin1"
text <- iconv(text,'latin1', 'ascii', sub = '')
#making a vector that will store each score tweet value
scoretweet <- numeric(length(text))
for (i in 1:length(text)){
#Transform text to lower case
text <- tolower(text)
#Split the tweet in words
tweetsplit <- str_split(text[i]," ")[[1]]
#Find the positions of the words in the Tweet in the dictionary
m <- match(tweetsplit, dictionary$Word)
#Which words are present in the dictionary?
present <- !is.na(m)
#Of the words that are present, select their valence
wordvalences <- dictionary$VALENCE[m[present]]
#Compute the mean valence of the tweet
scoretweet[i] <- mean(wordvalences, na.rm=TRUE)
#Handle the case when none of the words is in the dictionary
if (is.na(scoretweet[i])) scoretweet[i] <- 0 else scoretweet[i] <- scoretweet[i]
}
head(scoretweet)
hist(scoretweet)
#adding the scoretweet to all_tweets
all_tweets["score_tweet"] = scoretweet
all_tweets
all_tweets
all_tweets
all_tweets["text", "score_tweet"]
all_tweets[, 3:4]
all_tweets[, 3:4]
#inspect the first few sentiment scores
print(all_tweets[,3][1])
print(all_tweets[,4][1])
#inspect the first few sentiment scores
print(all_tweets[,3][1])
print(all_tweets[,4][1])
print(all_tweets[,3][5])
print(all_tweets[,4][5])
#inspect the first few sentiment scores
print(all_tweets[,3][1])
print(all_tweets[,4][1])
print(all_tweets[,3][35])
print(all_tweets[,4][35])
#inspect the first few sentiment scores
print(all_tweets[,3][1])
print(all_tweets[,4][1])
print(" ")
print(all_tweets[,3][35])
print(all_tweets[,4][35])
print(" ")
print(all_tweets[,3][2500])
print(all_tweets[,4][2500])
#inspect the first few sentiment scores
print(all_tweets[,3][1])
print(all_tweets[,4][1])
print(" ")
print(all_tweets[,3][35])
print(all_tweets[,4][35])
print(" ")
print(all_tweets[,3][250])
print(all_tweets[,4][250])
#inspect the first few sentiment scores
print(all_tweets[,3][1])
print(all_tweets[,4][1])
print(" ")
print(all_tweets[,3][35])
print(all_tweets[,4][35])
print(" ")
print(all_tweets[,3][350])
print(all_tweets[,4][350])
setwd("/Users/konstantinlazarov/Documents/GitHub/SMWA_Performance/data")
save(Topics_Frame, file = "All_tweets_with_sentiment.RData")
setwd("/Users/konstantinlazarov/Documents/GitHub/SMWA_Performance/data")
save(all_tweets, file = "All_tweets_with_sentiment.RData")
text
text[0:100]
p_load(tidyverse,textclean, textstem, sentimentr, lexicon)
text[0:100] %>% sentiment()
text[0:100] %>% sentiment_by()
p_load(tidyverse,textclean, textstem, sentimentr, lexicon)
#ranging from -2 (very negative) to 1 (very positive)
test <- text[0:100] %>% sentiment_by()
#By default, the sentiment_by function downweights the zero for averaging. The reason is that you don’t want the neutral sentences to have a strong influence
hist(test$ave_sentiment)
text[0:100] %>% sentiment_by()
text[0:100] %>% sentiment_by(averaging.function = average_weighted_mixed_sentiment)
text[0:100] %>% sentiment_by()
text[0:100] %>% sentiment_by(averaging.function = average_weighted_mixed_sentiment)
text[0:100] %>% sentiment_by()
text[0:100] %>% sentiment_by(averaging.function = average_weighted_mixed_sentiment)
text[0:100] %>% sentiment_by()
text[0:100] %>% sentiment_by(averaging.function = average_mean)
text[0:100] %>% replace_emoticon() %>%
replace_word_elongation() %>%
sentiment_by()
#adding these scores to the base table
all_tweets["sentiment_by"] <- all_tweets$textreplace_emoticon() %>%
replace_word_elongation() %>%
sentiment_by()
#adding these scores to the base table
all_tweets["sentiment_by"] <- all_tweets$text %>% replace_emoticon() %>%
replace_word_elongation() %>%
sentiment_by()
all_tweets
text[0:100]%>%get_sentences()
text[0:100] %>% get_sentences() %>% replace_emoticon() %>%
replace_word_elongation() %>%
sentiment_by()
text[0:100] %>% get_sentences() %>% replace_emoticon() %>%
replace_word_elongation() %>%
sentiment_by()
text[100:1000] %>% get_sentences() %>% replace_emoticon() %>%
replace_word_elongation() %>%
sentiment_by()
