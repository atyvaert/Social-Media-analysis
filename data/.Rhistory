RETICULATE_MINICONDA_PATH=normalizePath("~/islr-miniconda", winslash = "/", mustWork = FALSE))
source(system.file("helpers", "install.R", package = "ISLR2"))
install_miniconda()
install_tensorflow()
print_py_config()
print_py_config()
q()
q()
row <- 1
mdcv <- xgb.train(data=dtrain,
booster = "gbtree",
objective = "binary:logistic",
max_depth = parameters_df$max_depth[row],
eta = parameters_df$eta[row],
subsample = parameters_df$subsample[row],
colsample_bytree = parameters_df$colsample_bytree[row],
min_child_weight = parameters_df$min_child_weight[row],
nrounds= 1300,
eval_metric = "auc",
early_stopping_rounds= 50,
print_every_n = 100,
watchlist = list(train= dtrain, val= dvalid)
install.packages(c("backports", "BH", "brew", "broom", "car", "carData", "cli", "conquer", "cpp11", "crayon", "curl", "data.table", "digest", "dplyr", "ellipsis", "evaluate", "fansi", "foreach", "generics", "gert", "glmnet", "glue", "haven", "highr", "hms", "igraph", "ISLR2", "ISOcodes", "iterators", "jsonlite", "keras", "knitr", "lifecycle", "lightgbm", "lme4", "lmtest", "magrittr", "maptools", "matrixStats", "mime", "mockery", "nloptr", "openssl", "openxlsx", "pillar", "pkgbuild", "psych", "quantreg", "R6", "Rcpp", "RcppArmadillo", "RcppParallel", "readr", "reticulate", "rio", "rlang", "Rttf2pt1", "sandwich", "slam", "sp", "statmod", "stringi", "tensorflow", "testthat", "tibble", "tidyr", "tidyselect", "TTR", "utf8", "vctrs", "withr", "xfun", "yaml", "zip"))
# create network using the statnet package, more specific the network package
netmat <- rbind(c(0,1,1,0,0,0,0,0,0,0),
c(1,0,1,0,0,0,0,0,0,0),
c(1,1,0,1,1,0,1,0,0,0),
c(0,0,1,0,1,0,0,0,0,0),
c(0,0,1,1,0,1,0,0,0,0),
c(0,0,0,0,1,0,1,0,0,0),
c(0,0,1,0,0,1,0,1,0,0),
c(0,0,0,0,0,0,1,0,1,1),
c(0,0,0,0,0,0,0,1,0,0),
c(0,0,0,0,0,0,0,1,0,0))
rownames(netmat) <- c("A","B","C","D","E","F","G","H","I","J")
colnames(netmat) <- c("A","B","C","D","E","F","G","H","I","J")
net <- network(netmat)
library(statnet)
net <- network(netmat)
# calculate the degree of each node
degree(net)
# calculate the closeness of each node, normalized by default
closeness(net, gmode = "graph")
# calculate the betweenness of each node
betweenness(net, gmode="graph")
# loading dataset
data("Moreno")
# p_load() is from the pacman package
library(pacman)
p_load(devtools)
# get the datasets from GitHub
p_load_current_gh("DougLuke/UserNetR")
# loading dataset
data("Moreno")
# use igraph: a lot of options for community mining and clustering
detach(package:statnet)
p_load(igraph)
# reload data if needed
data(Moreno)
iMoreno <- intergraph::asIgraph(Moreno)
# inspect the data
table(V(iMoreno)$gender)
# calculate modularity of clusters based on gender
modularity(iMoreno,V(iMoreno)$gender)
data(Facebook)
levels(factor(V(Facebook)$group))
# translate labels into numeric vector
grp_num <- as.numeric(factor(V(Facebook)$group))
# calculate modularity based on this vector
modularity(Facebook,grp_num)
# add color to a plot to easily detect clusters
V(Facebook)$color <- grp_num
plot(Facebook)
# calculate the clusters based on edge betweenness (Girvan-Newman)
cw <- cluster_edge_betweenness(iMoreno)
# show membership of the clusters
membership(cw)
# calculate modularity, no clustering variable is required: a 'communities' object is used (clusters already determined)
modularity(cw)
# to plot: first supply the community, secondly the network
plot(cw, iMoreno)
# perform Girvan-Newman on the Facebook data
fb <- cluster_edge_betweenness(Facebook)
# calculate modularity and store it
modu <- modularity(fb, Facebook)
# mimic slide 67 of presention for Lecture 1
e <- data.frame(Cluster1=c(0.2,0.05,0.025),
Cluster2=c(0.05,0.3,0.1),
Cluster3=c(0.025,0.1,0.15))
rownames(e) <- c("Cluster1","Cluster2","Cluster3")
a <- rowSums(e)
b <- colSums(e)
rand <- data.frame()
for (i in 1: length(a)){
for (j in 1:length(b)){
rand[i,j] <- a[i]*b[j]
}
}
# bad measure of cluster quality
(trace_e <- sum(diag(as.matrix(e))))
# good measure of cluster quality
(Q <- sum(diag(as.matrix(e))) - sum(diag(as.matrix(rand))))
# create the data and the network
names <- c('A','B','C','D','E','F','G','H','I','J')
master <- c(rep('DA',6), rep('OR',4))
Students <- data.frame(name=names, master=master)
StudentsNetwork <- data.frame(from=c('A','A','A','A','B','B','C','C','D','D','D', 'E','F','F','G','G','H','H','I'),
to=c('B','C','D','E','C','D','D','G','E','F', 'G','F','G','I','I','H','I','J','J'),
label=c(rep('dd',7),'do','dd','dd','do','dd','do','do',rep('oo',5)))
names <- c('Tom', 'Arno', 'Sofie', 'Jan', 'Karen', 'Laura')
master <- c(rep('B', 3), rep('E',3))
names <- c("Tom", "Arno", "Sofie", "Jan", "Karen", "Laura")
master <- c(rep('B', 3), rep('E',3))
Students <- data.frame(name = names, master = masters)
names <- c("Tom", "Arno", "Sofie", "Jan", "Karen", "Laura")
masters <- c(rep('B', 3), rep('E',3))
Students <- data.frame(name = names, master = masters)
StudentsNetwork <- data.frame(from = c("Tom", "Sofie", "Karen"),
to = c("Arno", "Jan", "Laura"),
label = c("bb", "be", "ee"))
rm(masters)
master <- c(rep('B', 3), rep('E',3))
g <- graph_from_data_frame(StudentsNetwork, directed = FALSE)
V(g)$master <- Students$master
layout <- layout_nicely(g)
plot.igraph(g,
edge.label = NA,
edge.color = ifelse(StudentsNetwork$label == "bb", "blue",
ifelse(StudentsNetwork$label == "ee", "darkgreen", "red")))
plot.igraph(g,
edge.label = NA,
edge.color = ifelse(StudentsNetwork$label == "bb", "blue",
ifelse(StudentsNetwork$label == "ee", "darkgreen", "red")),
layout = layout,
vertex.label = V(g)$name,
vertex.color = ifelse(V(g)$master == 'B', "blue", "darkgreen"),
vertex.label.color = "white")
g <- graph_from_data_frame(StudentsNetwork, directed = FALSE)
V(g)$master <- Students$master
layout <- layout_nicely(g)
plot.igraph(g,
edge.label = NA,
edge.color = ifelse(StudentsNetwork$label == "bb", "blue",
ifelse(StudentsNetwork$label == "ee", "darkgreen", "red")),
layout = layout,
vertex.label = V(g)$name,
vertex.color = ifelse(V(g)$master == 'B', "blue", "darkgreen"),
vertex.label.color = "white")
library(igraph)
plot.igraph(g,
edge.label = NA,
edge.color = ifelse(StudentsNetwork$label == "bb", "blue",
ifelse(StudentsNetwork$label == "ee", "darkgreen", "red")),
layout = layout,
vertex.label = V(g)$name,
vertex.color = ifelse(V(g)$master == 'B', "blue", "darkgreen"),
vertex.label.color = "white")
names <- c("Tom", "Arno", "Sofie", "Jan", "Karen", "Laura")
master <- c(rep('B', 3), rep('E',3))
Students <- data.frame(name = names, master = masters)
Students <- data.frame(name = names, master = master)
StudentsNetwork <- data.frame(from = c("Tom", "Sofie", "Karen"),
to = c("Arno", "Jan", "Laura"),
label = c("bb", "be", "ee"))
g <- graph_from_data_frame(StudentsNetwork, directed = FALSE)
V(g)$master <- Students$master
layout <- layout_nicely(g)
plot.igraph(g,
edge.label = NA,
edge.color = ifelse(StudentsNetwork$label == "bb", "blue",
ifelse(StudentsNetwork$label == "ee", "darkgreen", "red")),
layout = layout,
vertex.label = V(g)$name,
vertex.color = ifelse(V(g)$master == 'B', "blue", "darkgreen"),
vertex.label.color = "white")
dev.off()
plot.igraph(g,
edge.label = NA,
edge.color = ifelse(StudentsNetwork$label == "bb", "blue",
ifelse(StudentsNetwork$label == "ee", "darkgreen", "red")),
layout = layout,
vertex.label = V(g)$name,
vertex.color = ifelse(V(g)$master == 'B', "blue", "darkgreen"),
vertex.label.color = "white")
plot.igraph(g,
edge.label = NA,
edge.color = ifelse(StudentsNetwork$label == "bb", "blue",
ifelse(StudentsNetwork$label == "ee", "darkgreen", "red")),
layout = layout,
vertex.label = V(g)$name,
vertex.color = ifelse(V(g)$master == 'B', "blue", "darkgreen"),
vertex.label.color = "white")
n_b <- length(which(V(g)$master == 'B'))
m_be <- length(which(E(g)$label == 'be'))
N <- length(V(g))
M <- length(E(g))
p <- 2*M/(N*(N-1))
n_e <- length(which(V(g)$master == 'E'))
m_ee <- length(which(E(g)$label == 'ee'))
m_bb <- length(which(E(g)$label == 'bb'))
N <- length(V(g))
M <- length(E(g))
p <- 2*M/(N*(N-1))
# calculate heterophilicity
(H <- m_be / bar_m_be)
# expected number of cross label edges in a random net
bar_m_be <- n_b*n_e*p
# calculate heterophilicity
(H <- m_be / bar_m_be)
bar_m_bb <- n_b * (n_b - 1) * p
(D_bb <- m_bb / bar_m_bb)
bar_m_ee <- n_e * (n_e - 1) * p
(D_ee <- m_ee / bar_m_ee)
bar_m_bb <- n_b * (n_b - 1)/2 * p
(D_bb <- m_bb / bar_m_bb)
bar_m_ee <- n_e * (n_e - 1)/2 * p
(D_ee <- m_ee / bar_m_ee)
# load node2vec
p_load(node2vec)
# load node2vec
p_load(node2vec)
# p_load() is from the pacman package
library(pacman)
library(pacman)
# load node2vec
p_load(node2vec)
# 1. Create the Data and the Network
names <- c('A','B','C','D','E','F','G','H','I','J')
master <- c(rep('DA',6), rep('OR',4))
Students <- data.frame(name=names, master=master)
StudentsNetwork <- data.frame(from=c('A','A','A','A','B','B','C','C','D','D','D', 'E','F','F','G','G','H','H','I'),
to=c('B','C','D','E','C','D','D','G','E','F', 'G','F','G','I','I','H','I','J','J'),
label=c(rep('dd',7),'do','dd','dd','do','dd','do','do',rep('oo',5)))
# 2. Plot the Network
g <- graph_from_data_frame(StudentsNetwork, directed = FALSE)
library(statnet)
# 2. Plot the Network
g <- graph_from_data_frame(StudentsNetwork, directed = FALSE)
library(igraph)
# 2. Plot the Network
g <- graph_from_data_frame(StudentsNetwork, directed = FALSE)
V(g)$master <- as.character(Students$master)
layout <- layout_nicely(g)
plot.igraph(g,
edge.label = NA,
edge.color = ifelse(StudentsNetwork$label == "dd", "blue", ifelse(StudentsNetwork$label == "do", "red", "darkgreen")),
layout = layout,
vertex.label = V(g)$name,
vertex.color = ifelse(V(g)$master == "DA", "blue", "darkgreen"),
vertex.label.color = 'white')
# Calculate number of nodes
N <- length(V(g))
n_da <- length(which(V(g)$master=='DA'))
n_or <- length(which(V(g)$master=='OR'))
# Calculate number of edges
M <- length(E(g))
m_dd <- length(which(E(g)$label=='dd'))
m_oo <- length(which(E(g)$label=='oo'))
m_do <- length(which(E(g)$label=='do'))
# Calculate connectedness
p <- 2*M/(N*(N-1))        #  = graph.density(g)
# expected number of cross label edges in a random net
bar_m_do <- n_da*n_or*p
# calculate heterophilicity
H <- m_do/bar_m_do
H
# expected number of cross label edges in a random net
bar_m_be <- n_b*n_e*p
# calculate heterophilicity
(H <- m_be / bar_m_be)
# for DA
bar_m_dd <- n_da*(n_da-1)/2*p
D_dd <- m_dd/bar_m_dd
D_dd
# for OR
bar_m_oo <- n_or*(n_or-1)/2*p
D_oo <- m_oo/bar_m_oo
D_oo
# calculate node2vec using return parameter p = 1, and in-out parameter q = 1
emb <- node2vecR(StudentsNetwork[,-3], p=1,q=1,num_walks=5,walk_length=5,dim=5)
# inspect corresponding embeddings
str(emb)
# add rowname to embedding matrix to know which node we are talking about
emb_name <- data.frame(node = row.names(emb), emb)
row.names(emb)
# calculate the cosine similarity between node G and all other nodes
cos_sim <- sim2(x = as.matrix(emb), y = as.matrix(emb[emb_name$node == "G",, drop = FALSE]), method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 10)
# statnet is a network package
library(statnet)
# p_load() is from the pacman package
library(pacman)
# load igraph package
library(igraph)
# calculate the cosine similarity between node G and all other nodes
cos_sim <- sim2(x = as.matrix(emb), y = as.matrix(emb[emb_name$node == "G",, drop = FALSE]), method = "cosine", norm = "l2")
library(text2vec)
install.packages(text2vec)
install.packages("text2vec")
library(text2vec)
# calculate the cosine similarity between node G and all other nodes
cos_sim <- sim2(x = as.matrix(emb), y = as.matrix(emb[emb_name$node == "G",, drop = FALSE]), method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 10)
# calculate node2vec using return parameter p = 0.5, and in-out parameter q = 2
emb <- node2vecR(StudentsNetwork[,-3], p=0.5,q=2,num_walks=5,walk_length=5,dim=5)
# inspect corresponding embeddings
str(emb)
# add rowname to embedding matrix to know which node we are talking about
emb_name <- data.frame(node = row.names(emb), emb)
# calculate the cosine similarity between node G and all other nodes
cos_sim <- sim2(x = as.matrix(emb), y = as.matrix(emb[emb_name$node == "G",, drop = FALSE]), method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 10)
p_load(httr, rtweet, tidyverse, wordcloud2, tm, tidytext)
get_token()
search.string <- "#Tesla"
tweets <- search_tweets(search.string, n = 1000,
include_rts = FALSE,
retryonratelimit = FALSE,
lang = 'en')
tweets_data(tweets)
text <- tweets_data(tweets) %>% pull(text)
p_load(tm,wordcloud, wordcloud2)
tdm <- TermDocumentMatrix(Corpus(VectorSource(text)))
m <- as.matrix(tdm)
library(pacman)
p_load(httr, rtweet, tidyverse, wordcloud2, tm, tidytext)
get_token()
search.string <- "#Tesla"
tweets <- search_tweets(search.string, n = 1000,
include_rts = FALSE,
retryonratelimit = FALSE,
lang = 'en')
tweets_data(tweets)
text <- tweets_data(tweets) %>% pull(text)
p_load(tm,wordcloud, wordcloud2)
tdm <- TermDocumentMatrix(Corpus(VectorSource(text)))
m <- as.matrix(tdm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- tibble(word = names(v),freq=v)
d <- d %>% filter(word != tolower(search.string)) %>% arrange(desc(freq))
options(warn=-1) #turn warnings off
wordcloud2(d)
options(warn=0) #turn warnings back on
# now with preprocessing
p_load(SnowballC, slam, tm)
text <- Corpus(VectorSource(text))
unique_word_count <- function(data){
content <- character(length(data))
if (any(class(data) %in% c("VCorpus", "Corpus","SimpleCorpus"))) {
for (i in 1:length(data)) content[i] <- data[[i]]$content
} else {
content <- data
}
uniquewords <- unique(unlist(map(
str_split(as.character(content)," "),
unique)))
length(uniquewords)
}
unique_word_count(text)
text_preprocessed <- Corpus(VectorSource(text)) %>%
tm_map(content_transformer(str_to_lower)) %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace)
unique_word_count(text_preprocessed)
myCorpus <- Corpus(VectorSource(text_preprocessed))
myStopwords <- c(stopwords('english'))
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
unique_word_count(text_preprocessed)
tdm <- TermDocumentMatrix(myCorpus)
m <- as.matrix(tdm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- tibble(word = names(v),freq=v)
d <- d %>% filter(word != tolower(search.string)) %>% arrange(desc(freq))
d <- d %>% filter(word != "the") %>% arrange(desc(freq))
d <- d %>% filter(word != "and") %>% arrange(desc(freq))
d <- d %>% filter(word != "with") %>% arrange(desc(freq))
d <- d %>% filter(word != "for") %>% arrange(desc(freq))
d <- d %>% filter(word != "you") %>% arrange(desc(freq))
d <- d %>% filter(word != "tesla") %>% arrange(desc(freq))
d <- d %>% filter(word != "elon") %>% arrange(desc(freq))
d <- d %>% filter(word != "musk") %>% arrange(desc(freq))
d <- d %>% filter(word != "elonmusk") %>% arrange(desc(freq))
options(warn=-1) #turn warnings off
wordcloud_tesla <- wordcloud2(d)
options(warn=0) #turn warnings back on
wordcloud_tesla <- wordcloud2(d)
# wordgraph
text <- iconv(text,'latin1', 'ascii', sub = '')
create_document_term_matrix <- function(data){
p_load(SnowballC, tm)
myCorpus <- Corpus(VectorSource(data))
cat("Transform to lower case \n")
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
cat("Remove punctuation \n")
myCorpus <- tm_map(myCorpus, removePunctuation)
cat("Remove numbers \n")
myCorpus <- tm_map(myCorpus, removeNumbers)
cat("Remove stopwords \n")
myStopwords <- c(stopwords('english'))
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
# cat("Stem corpus \n")
# myCorpus = tm_map(myCorpus, stemDocument);
# myCorpus = tm_map(myCorpus, stemCompletion, dictionary=dictCorpus);
cat("Create document by term matrix \n")
myDtm <- DocumentTermMatrix(myCorpus, control = list(wordLengths = c(2, Inf)))
myDtm
}
doc_term_mat <- create_document_term_matrix(text)
create_adjacency_matrix <- function(object, probs=0.99){
#object = output from function create_document_term_matrix (a document by term matrix)
#probs = select only vertexes with degree greater than or equal to quantile given by the value of probs
cat("Create adjacency matrix \n")
p_load(sna)
mat <- as.matrix(object)
mat[mat >= 1] <- 1 #change to boolean (adjacency) matrix
Z <- t(mat) %*% mat
cat("Apply filtering \n")
ind <- sna::degree(as.matrix(Z),cmode = "indegree") >= quantile(sna::degree(as.matrix(Z),cmode = "indegree"),probs=0.99)
#ind <- sna::betweenness(as.matrix(Z)) >= quantile(sna::betweenness(as.matrix(Z)),probs=0.99)
Z <- Z[ind,ind]
cat("Resulting adjacency matrix has ",ncol(Z)," rows and columns \n")
dim(Z)
list(Z=Z,termbydocmat=object,ind=ind)
}
adj_mat <- create_adjacency_matrix(doc_term_mat)
adj_mat[[1]][1:5,1:5]
plot_network <- function(object){
#Object: output from the create_adjacency_matrix function
#Create graph from adjacency matrix
p_load(igraph)
g <- graph.adjacency(object$Z, weighted=TRUE, mode ='undirected')
g <- simplify(g)
#Set labels and degrees of vertices
V(g)$label <- V(g)$name
V(g)$degree <- igraph::degree(g)
layout <- layout.auto(g)
opar <- par()$mar; par(mar=rep(0, 4)) #Give the graph lots of room
#Adjust the widths of the edges and add distance measure labels
#Use 1 - binary (?dist) a proportion distance of two vectors
#The binary distance (or Jaccard distance) measures the dissimilarity, so 1 is perfect and 0 is no overlap (using 1 - binary)
edge.weight <- 7  #A maximizing thickness constant
z1 <- edge.weight*(1-dist(t(object$termbydocmat)[object$ind,], method="binary"))
E(g)$width <- c(z1)[c(z1) != 0] #Remove 0s: these won't have an edge
clusters <- spinglass.community(g)
cat("Clusters found: ", length(clusters$csize),"\n")
cat("Modularity: ", clusters$modularity,"\n")
plot(g, layout=layout, vertex.color=rainbow(4)[clusters$membership], vertex.frame.color=rainbow(4)[clusters$membership] )
}
plot_network(adj_mat)
load("~/GitHub/SMWA_Performance/data/Scrape5_03Till13_03_Starbucks.RData")
tweets2
library(pacman)
p_load(twitteR)
tweets2
rm(list=ls())
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\GitHub\\SMWA_Performance\\data")
load("First_Scrape9_03.Rdata")
tweets1 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
#load("Scrape_09_03.Rdata")
#tweets2 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])   # there is something wrong with this file
load("Scrape1_10_03.Rdata")
tweets3 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_13_03.Rdata")
tweets4 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_14_03.Rdata")
tweets5 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_15_03.Rdata")
tweets6 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_16_03.Rdata")
tweets7 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape2_16_03.Rdata")
tweets8 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape3_16_03.Rdata")
tweets9 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape4_16_03.Rdata")
tweets10 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape5_16_03.Rdata")
tweets11 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape5_03.Rdata")
tweets12 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape3_22_03.Rdata")
tweets13 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape3_19_03.Rdata")
tweets14 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape3_18_03.Rdata")
tweets15 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape2_22_03.Rdata")
tweets16 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape2_19_03.Rdata")
tweets17 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape2_18_03.Rdata")
tweets18 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_22_03.Rdata")
tweets19 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_19_03.Rdata")
tweets20 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape_17_03_tweets2.Rdata")
tweets21 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape_17_03.Rdata")
tweets22 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape_18_03.Rdata")
tweets23 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape2_22_03.Rdata")
tweets24 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape_24_03.Rdata")
tweets25 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
all_tweets <- rbind(tweets1, tweets3, tweets4, tweets5, tweets6, tweets7, tweets8, tweets9, tweets10, tweets11, tweets12, tweets13, tweets14,
tweets15, tweets16, tweets17, tweets18, tweets19, tweets20, tweets21, tweets22, tweets23, tweets24, tweets25)
colnames(all_tweets) <- c("user_id", "date", "text")
sum(duplicated(all_tweets))
all_tweets <- all_tweets[!duplicated(all_tweets), ]
