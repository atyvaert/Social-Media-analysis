ggplot(aes(x = topics, y = UMass)) + geom_line()
#Best performance with minimal amount of topics
(optimal_settings <- grid %>% filter(UMass == max(UMass)))
#Visualize impact K with Dirichlet parameters assumed stable
grid %>%
filter(alpha==0.4 & beta==0.2) %>%
ggplot(aes(x = topics, y = UMass)) + geom_line()
optimal_settings
#Best performance with minimal amount of topics
(optimal_settings <- grid %>% filter(UMass == max(UMass)))
#Visualize impact K with Dirichlet parameters assumed stable
grid %>%
filter(alpha==0.4 & beta==0.2) %>%
ggplot(aes(x = topics, y = UMass)) + geom_line()
lda <- LDA(dtm_new,
control = list(alpha = optimal_settings$alpha,
estimate.beta = optimal_settings$beta),
k = optimal_settings$topics)
terms(lda, 20)
#We can also derive how the topics are distributed per document:
lda@gamma[1:20,]
#Or tidy
#see ?tidy.LDA for more info
tidy(lda, matrix = 'gamma')
#Or tidy
#see ?tidy.LDA for more info
tidy(lda, matrix = 'gamma')
optimal_settings$topics
lda <- LDA(dtm_new,
control = list(alpha = optimal_settings$alpha,
estimate.beta = optimal_settings$beta),
k = optimal_settings$topics)
terms(lda, 20)
#We can also derive how the topics are distributed per document:
lda@gamma[1:20,]
#Or tidy
#see ?tidy.LDA for more info
tidy(lda, matrix = 'gamma')
lda$topic
lda@topic
lda <- LDA(dtm_new,
control = list(alpha = optimal_settings$alpha,
estimate.beta = optimal_settings$beta),
k = optimal_settings$topics)
terms(lda, 20)
#We can also derive how the topics are distributed per document:
lda@gamma[1:20,]
#Or tidy
#see ?tidy.LDA for more info
tidy(lda, matrix = 'gamma')
lda@gamma
sample <- sample[rowTotals > 0,]
sample[,c('topic1','topic2', "topic3", "topic4",
"topic5", "topic6", "topic7", "topic8", "topic9", "topic10")] <- lda@gamma
#Make a tibble ready for visualization
#We want one row per listing together with the top topic and its distribution
visualisation_set <- sample %>%
group_by(listing_id) %>%
summarise_at(c('topic1','topic2'), mean, na.rm = TRUE) %>%
pivot_longer(c('topic1','topic2'), names_to = "topic", values_to = "cnt") %>%
group_by(listing_id) %>%
slice_max(cnt)
#Import spatial information
coordinates <- read_csv('listings.csv')
merged_set <- coordinates %>%
mutate(listing_id = id) %>%
inner_join(visualisation_set, 'listing_id')
get_stamenmap(bbox = c(left = -122.7276, bottom = 37.6373, right = -121.9688, top = 37.8881),
zoom = 10, maptype = "toner-lite") %>% ggmap()
qmplot(longitude, latitude,
data = merged_set,
maptype = "toner-lite",
size = cnt, color = topic)
coordinates
merged_set <- coordinates %>%
mutate(listing_id = id) %>%
inner_join(visualisation_set, 'listing_id')
qmplot(longitude, latitude,
data = merged_set,
maptype = "toner-lite",
size = cnt, color = topic)
optimal_settings$topics
lda <- LDA(dtm_new,
control = list(alpha = optimal_settings$alpha,
estimate.beta = optimal_settings$beta),
k = optimal_settings$topics)
terms(lda, 20)
#We can also derive how the topics are distributed per document:
lda@gamma[1:20,]
#Or tidy
#see ?tidy.LDA for more info
tidy(lda, matrix = 'gamma')
terms(lda, 20)
lda@gamma[1:20,]
tidy(lda, matrix = 'gamma')
terms(lda, 20)
optimal_settings$topics)
optimal_settings$topics
terms(lda, 20)
tidy(lda, matrix = 'gamma')
optimal_settings$topics
terms(lda, 20)
#Or tidy
#see ?tidy.LDA for more info
tidy(lda, matrix = 'gamma')
terms(lda, 20)
View(visualisation_set)
sample
lda@gamma
sample
visualisation_set <- sample %>%
group_by(listing_id) %>%
summarise_at(c("topic3", "topic4",
"topic5", "topic6", "topic7", "topic8", "topic9", "topic10"), mean, na.rm = TRUE) %>%
pivot_longer(c ("topic3", "topic4",
"topic5", "topic6", "topic7", "topic8", "topic9", "topic10"), names_to = "topic", values_to = "cnt") %>%
group_by(listing_id) %>%
slice_max(cnt)
coordinates <- read_csv('listings.csv')
merged_set <- coordinates %>%
mutate(listing_id = id) %>%
inner_join(visualisation_set, 'listing_id')
get_stamenmap(bbox = c(left = -122.7276, bottom = 37.6373, right = -121.9688, top = 37.8881),
zoom = 10, maptype = "toner-lite") %>% ggmap()
qmplot(longitude, latitude,
data = merged_set,
maptype = "toner-lite",
size = cnt, color = topic)
sample <- sample[rowTotals > 0,]
#Make a tibble ready for visualization
#We want one row per listing together with the top topic and its distribution
visualisation_set <- sample %>%
group_by(listing_id) %>%
summarise_at(c('topic1','topic2', "topic3", "topic4",
"topic5", "topic6", "topic7", "topic8", "topic9", "topic10"), mean, na.rm = TRUE) %>%
pivot_longer(c('topic1','topic2'), names_to = "topic", values_to = "cnt") %>%
group_by(listing_id) %>%
slice_max(cnt)
sample <- sample[rowTotals > 0,]
if (!require("pacman")) install.packages("pacman") ; require("pacman")
p_load(tidyverse, topicdoc, topicmodels, tm, textcat, rlang, tidytext, ggmap)
#p_install_gh("dkahle/ggmap", ref = "tidyup")
reviews <- read_csv('reviews.csv')
reviews %>% glimpse()
set.seed(123)
sampled_listing_ids <- sample(unique(reviews$listing_id),
round(length(unique(reviews$listing_id))/100),
replace = FALSE)
sample <- reviews %>% filter(listing_id %in% sampled_listing_ids)
#Transform columns to workable format (should be okay if you read it in as tibble)
sample <- sample %>%
mutate_if(is.factor, as.character) %>%
mutate(date=as.Date(date, format = "%Y-%m-%d"))
sample$language <- textcat(sample$comments)
sample <- sample %>% filter(language == 'english')
corpus1 <- Corpus(VectorSource(sample$comments))
dtm1 <- DocumentTermMatrix(corpus1)
inspect(dtm1)
lda_basic <- LDA(dtm1,
control = list(seed = 33),
k = 5)
terms(lda_basic, 20)
review_corpus <- VCorpus(VectorSource(sample$comments))
review_corpus <- review_corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(removeWords, c("the", "and", stopwords("english"))) %>%
tm_map(stripWhitespace)
#Include both bigrams AND unigrams
#BigramTokenizer <- function(x) {
# c(unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE),
#  unlist(lapply(ngrams(words(x), 1), paste, collapse = " "), use.names = FALSE))
#}
#Note how we specify the tokenizer, compared to the default word (unigram) tokenizer
#final_dtm <- DocumentTermMatrix(review_corpus,
#                                control = list(tokenize = BigramTokenizer))
final_dtm <- DocumentTermMatrix(review_corpus)
final_dtm
final_dtm <- removeSparseTerms(final_dtm, 0.90)
final_dtm
#Inspect terms: looks good
final_dtm$dimnames$Terms[1:100]
#Create initial LDA model without optimized parameter settings
lda_full <- LDA(final_dtm,
control = list(seed = 33),
k = 5)
rowTotals <- apply(final_dtm , 1, sum) #Find the sum of words in each Document
dtm_new   <- final_dtm[rowTotals> 0, ]
(length(final_dtm$dimnames$Docs)-length(dtm_new$dimnames$Docs))
#New attempt
lda_full <- LDA(dtm_new,
control = list(seed = 33),
k = 5)
#See the top 20 terms associated with each of the topics
terms(lda_full, 20)
topics <- seq(2,10,1)
alpha <- seq(0.2,0.8,0.2)
beta <- seq(0.2,0.8,0.2)
grid <- expand.grid(topics, alpha, beta)
colnames(grid) <- c('topics', 'alpha', 'beta')
grid$UMass <- rep(-99999999999, nrow(grid))
#This takes a while ==> best to load fitted grid
for (i in 1:nrow(grid)){
K <- grid[i,'topics']
a <- grid[i,'alpha']
b <- grid[i,'beta']
lda <- LDA(dtm_new, control = list(alpha = a, estimate.beta = b), k = K)
grid[i,'UMass'] <- mean(topic_coherence(lda, dtm_new, top_n_tokens = 30))
}
save(grid, file = 'gridLDA.Rdata')
#Best performance with minimal amount of topics
(optimal_settings <- grid %>% filter(UMass == max(UMass)))
#Visualize impact K with Dirichlet parameters assumed stable
grid %>%
filter(alpha==0.4 & beta==0.2) %>%
ggplot(aes(x = topics, y = UMass)) + geom_line()
#Best performance with minimal amount of topics
(optimal_settings <- grid %>% filter(UMass == max(UMass)))
#Visualize impact K with Dirichlet parameters assumed stable
grid %>%
filter(alpha==0.2 & beta==0.4) %>%
ggplot(aes(x = topics, y = UMass)) + geom_line()
optimal_settings
optimal_settings$topics
lda <- LDA(dtm_new,
control = list(alpha = optimal_settings$alpha,
estimate.beta = optimal_settings$beta),
k = optimal_settings$topics)
terms(lda, 20)
#We can also derive how the topics are distributed per document:
lda@gamma[1:20,]
#Or tidy
#see ?tidy.LDA for more info
tidy(lda, matrix = 'gamma')
sample <- sample[rowTotals > 0,]
sample[,c('topic1','topic2', "topic3", "topic4",
"topic5", "topic6", "topic7", "topic8")] <- lda@gamma
#Make a tibble ready for visualization
#We want one row per listing together with the top topic and its distribution
visualisation_set <- sample %>%
group_by(listing_id) %>%
summarise_at(c('topic1','topic2', "topic3", "topic4",
"topic5", "topic6", "topic7", "topic8"), mean, na.rm = TRUE) %>%
pivot_longer(c('topic1','topic2',, "topic3", "topic4",
"topic5", "topic6", "topic7", "topic8"), names_to = "topic", values_to = "cnt") %>%
group_by(listing_id) %>%
slice_max(cnt)
#Import spatial information
coordinates <- read_csv('listings.csv')
merged_set <- coordinates %>%
mutate(listing_id = id) %>%
inner_join(visualisation_set, 'listing_id')
get_stamenmap(bbox = c(left = -122.7276, bottom = 37.6373, right = -121.9688, top = 37.8881),
zoom = 10, maptype = "toner-lite") %>% ggmap()
qmplot(longitude, latitude,
data = merged_set,
maptype = "toner-lite",
size = cnt, color = topic)
#Visualize impact K with Dirichlet parameters assumed stable
grid %>%
filter(alpha==0.2 & beta==0.4) %>%
ggplot(aes(x = topics, y = UMass)) + geom_line()
#Visualize impact K with Dirichlet parameters assumed stable
grid %>%
filter(alpha==0.2 & beta==0.2) %>%
ggplot(aes(x = topics, y = UMass)) + geom_line()
#Visualize impact K with Dirichlet parameters assumed stable
grid %>%
filter(alpha==0.2 & beta==0.4) %>%
ggplot(aes(x = topics, y = UMass)) + geom_line()
#Best performance with minimal amount of topics
(optimal_settings <- grid %>% filter(UMass == max(UMass)))
#Visualize impact K with Dirichlet parameters assumed stable
grid %>%
filter(alpha==0.4 & beta==0.4) %>%
ggplot(aes(x = topics, y = UMass)) + geom_line()
#Best performance with minimal amount of topics
(optimal_settings <- grid %>% filter(UMass == max(UMass)))
#Visualize impact K with Dirichlet parameters assumed stable
grid %>%
filter(alpha==0.2 & beta==0.4) %>%
ggplot(aes(x = topics, y = UMass)) + geom_line()
terms(lda, 20)
terms(lda, 20)
lda <- LDA(dtm_new,
control = list(alpha = optimal_settings$alpha,
estimate.beta = optimal_settings$beta),
k = 4)
terms(lda, 20)
#We can also derive how the topics are distributed per document:
lda@gamma[1:20,]
#Or tidy
#see ?tidy.LDA for more info
tidy(lda, matrix = 'gamma')
sample <- sample[rowTotals > 0,]
#sample <- sample[rowTotals > 0,]
sample[,c('topic1','topic2', "topic3", "topic4")] <- lda@gamma
#Make a tibble ready for visualization
#We want one row per listing together with the top topic and its distribution
visualisation_set <- sample %>%
group_by(listing_id) %>%
summarise_at(c('topic1','topic2', "topic3", "topic4"), mean, na.rm = TRUE) %>%
pivot_longer(c('topic1','topic2',, "topic3", "topic4"), names_to = "topic", values_to = "cnt") %>%
group_by(listing_id) %>%
slice_max(cnt)
#Import spatial information
coordinates <- read_csv('listings.csv')
merged_set <- coordinates %>%
mutate(listing_id = id) %>%
inner_join(visualisation_set, 'listing_id')
get_stamenmap(bbox = c(left = -122.7276, bottom = 37.6373, right = -121.9688, top = 37.8881),
zoom = 10, maptype = "toner-lite") %>% ggmap()
qmplot(longitude, latitude,
data = merged_set,
maptype = "toner-lite",
size = cnt, color = topic)
terms(lda,20)
if (!require("pacman")) install.packages("pacman") ; require("pacman")
p_load(tidyverse, topicdoc, topicmodels, tm, textcat, rlang, tidytext, ggmap)
#p_install_gh("dkahle/ggmap", ref = "tidyup")
reviews <- read_csv('reviews.csv')
reviews %>% glimpse()
reviews
sampled_listing_ids <- sample(unique(reviews$listing_id),
round(length(unique(reviews$listing_id))/10),
replace = FALSE)
sampled_listing_ids
sample <- reviews %>% filter(listing_id %in% sampled_listing_ids)
sample
sample <- sample %>%
mutate_if(is.factor, as.character) %>%
mutate(date=as.Date(date, format = "%Y-%m-%d"))
sample$language <- textcat(sample$comments)
sample <- sample %>% filter(language == 'english')
set.seed(123)
sampled_listing_ids <- sample(unique(reviews$listing_id),
round(length(unique(reviews$listing_id))/20),
replace = FALSE)
sample <- reviews %>% filter(listing_id %in% sampled_listing_ids)
#Transform columns to workable format (should be okay if you read it in as tibble)
sample <- sample %>%
mutate_if(is.factor, as.character) %>%
mutate(date=as.Date(date, format = "%Y-%m-%d"))
sample
sample$language <- textcat(sample$comments)
sample <- sample %>% filter(language == 'english')
corpus1 <- Corpus(VectorSource(sample$comments))
dtm1 <- DocumentTermMatrix(corpus1)
inspect(dtm1)
lda_basic <- LDA(dtm1,
control = list(seed = 33),
k = 5)
terms(lda_basic, 20)
review_corpus <- VCorpus(VectorSource(sample$comments))
review_corpus <- review_corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(removeWords, c("the", "and", stopwords("english"))) %>%
tm_map(stripWhitespace)
#Include both bigrams AND unigrams
#BigramTokenizer <- function(x) {
# c(unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE),
#  unlist(lapply(ngrams(words(x), 1), paste, collapse = " "), use.names = FALSE))
#}
#Note how we specify the tokenizer, compared to the default word (unigram) tokenizer
#final_dtm <- DocumentTermMatrix(review_corpus,
#                                control = list(tokenize = BigramTokenizer))
final_dtm <- DocumentTermMatrix(review_corpus)
final_dtm
final_dtm <- removeSparseTerms(final_dtm, 0.90)
final_dtm
#Inspect terms: looks good
final_dtm$dimnames$Terms[1:100]
#Create initial LDA model without optimized parameter settings
lda_full <- LDA(final_dtm,
control = list(seed = 33),
k = 5)
rowTotals <- apply(final_dtm , 1, sum) #Find the sum of words in each Document
dtm_new   <- final_dtm[rowTotals> 0, ]
(length(final_dtm$dimnames$Docs)-length(dtm_new$dimnames$Docs))
#New attempt
lda_full <- LDA(dtm_new,
control = list(seed = 33),
k = 5)
#See the top 20 terms associated with each of the topics
terms(lda_full, 20)
topics <- seq(2,10,1)
alpha <- seq(0.2,0.8,0.2)
beta <- seq(0.2,0.8,0.2)
grid <- expand.grid(topics, alpha, beta)
colnames(grid) <- c('topics', 'alpha', 'beta')
grid$UMass <- rep(-99999999999, nrow(grid))
#This takes a while ==> best to load fitted grid
for (i in 1:nrow(grid)){
K <- grid[i,'topics']
a <- grid[i,'alpha']
b <- grid[i,'beta']
lda <- LDA(dtm_new, control = list(alpha = a, estimate.beta = b), k = K)
grid[i,'UMass'] <- mean(topic_coherence(lda, dtm_new, top_n_tokens = 30))
}
save(grid, file = 'gridLDA.Rdata')
grid
#Best performance with minimal amount of topics
(optimal_settings <- grid %>% filter(UMass == max(UMass)))
#Visualize impact K with Dirichlet parameters assumed stable
grid %>%
filter(alpha==0.2 & beta==0.4) %>%
ggplot(aes(x = topics, y = UMass)) + geom_line()
lda <- LDA(dtm_new,
control = list(alpha = optimal_settings$alpha,
estimate.beta = optimal_settings$beta),
k = 4)
terms(lda, 20)
#We can also derive how the topics are distributed per document:
lda@gamma[1:20,]
#Or tidy
#see ?tidy.LDA for more info
tidy(lda, matrix = 'gamma')
terms(lda, 20)
#Make a tibble ready for visualization
#We want one row per listing together with the top topic and its distribution
visualisation_set <- sample %>%
group_by(listing_id) %>%
summarise_at(c('topic1','topic2', "topic3", "topic4"), mean, na.rm = TRUE) %>%
pivot_longer(c('topic1','topic2',, "topic3", "topic4"), names_to = "topic", values_to = "cnt") %>%
group_by(listing_id) %>%
slice_max(cnt)
sample[,c('topic1','topic2', "topic3", "topic4")] <- lda@gamma
lda@gamma
#sample <- sample[rowTotals > 0,]
sample[,c('topic1','topic2', "topic3", "topic4")] <- lda@gamma
sample <- sample[rowTotals > 0,]
sample[,c('topic1','topic2', "topic3", "topic4")] <- lda@gamma
#Make a tibble ready for visualization
#We want one row per listing together with the top topic and its distribution
visualisation_set <- sample %>%
group_by(listing_id) %>%
summarise_at(c('topic1','topic2', "topic3", "topic4"), mean, na.rm = TRUE) %>%
pivot_longer(c('topic1','topic2',, "topic3", "topic4"), names_to = "topic", values_to = "cnt") %>%
group_by(listing_id) %>%
slice_max(cnt)
#Import spatial information
coordinates <- read_csv('listings.csv')
merged_set <- coordinates %>%
mutate(listing_id = id) %>%
inner_join(visualisation_set, 'listing_id')
get_stamenmap(bbox = c(left = -122.7276, bottom = 37.6373, right = -121.9688, top = 37.8881),
zoom = 10, maptype = "toner-lite") %>% ggmap()
qmplot(longitude, latitude,
data = merged_set,
maptype = "toner-lite",
size = cnt, color = topic)
########################################
## DESCPRITIVE ##
rm(list=ls()) #Clean the environment
#install.packages("twitteR")
library(twitteR)
search.string <- "#Starbucks"
help("searchTwitter")
#install.packages("twitteR")
library(twitteR)
consumer_key <- 'u7KJn5vc0eAhnEIAdxuEftRu6'
consumer_secret <- 'col5xyDsno07FrlY88M6tyHYTt5uEggJwN9A4AbQmVHO75ZfLU'
access_token <- '1492166932132474880-2bedVAngfyyRLo7Wldp5Kact5zHHHY'
access_secret <- 'ICzs9yOImM5PeKnlJnn5SZ9Q9fVFIRsftm35y0pIXIA7v'
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
#install.packages("twitteR")
library(twitteR)
consumer_key <- 'u7KJn5vc0eAhnEIAdxuEftRu6'
consumer_secret <- 'col5xyDsno07FrlY88M6tyHYTt5uEggJwN9A4AbQmVHO75ZfLU'
access_token <- '1492166932132474880-2bedVAngfyyRLo7Wldp5Kact5zHHHY'
access_secret <- 'ICzs9yOImM5PeKnlJnn5SZ9Q9fVFIRsftm35y0pIXIA7v'
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
#install.packages("twitteR")
library(twitteR)
consumer_key <- 'u7KJn5vc0eAhnEIAdxuEftRu6'
consumer_secret <- 'col5xyDsno07FrlY88M6tyHYTt5uEggJwN9A4AbQmVHO75ZfLU'
access_token <- '1492166932132474880-2bedVAngfyyRLo7Wldp5Kact5zHHHY'
access_secret <- 'ICzs9yOImM5PeKnlJnn5SZ9Q9fVFIRsftm35y0pIXIA7v'
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
########################################
## DESCPRITIVE ##
rm(list=ls()) #Clean the environment
setwd("/Users/konstantinlazarov/Desktop/SMWA/Group_Assignement")
# Install and load packages
if (!require("pacman")) install.packages("pacman") ; require("pacman")
p_load(tidyverse, rtweet, httpuv, stringr, qdap, httr, wordcloud2, tm, tidytext, wordcloud)
source('tokensandkeys.R')
token_good <- get_token()
search.string <- "#Starbucks"
tweets <- search_tweets(search.string, n = 13000,
include_rts = FALSE,
retryonratelimit = FALSE,
lang = 'en',
token = token_good)
setwd("/Users/konstantinlazarov/Documents/GitHub/SMWA_Performance/data")
save(tweets, file = "Scrape5_03Till13_03.RData")
save(tweets, file = "Scrape_18_03_RData")
tweets <- search_tweets(search.string, n = 10000,
include_rts = FALSE,
retryonratelimit = FALSE,
lang = 'en',
token = get_token())
save(tweets, file = "Scrape2_18_03Till13_03.RData")
tweets <- search_tweets(search.string, n = 10000,
include_rts = FALSE,
retryonratelimit = FALSE,
lang = 'en',
token = get_token())
save(tweets, file = "Scrape3_18_03Till13_03.RData")
