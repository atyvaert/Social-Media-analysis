options(warn=0) #turn warnings back on
# now with preprocessing
p_load(SnowballC, slam, tm)
text <- Corpus(VectorSource(text))
unique_word_count <- function(data){
content <- character(length(data))
if (any(class(data) %in% c("VCorpus", "Corpus","SimpleCorpus"))) {
for (i in 1:length(data)) content[i] <- data[[i]]$content
} else {
content <- data
}
uniquewords <- unique(unlist(map(
str_split(as.character(content)," "),
unique)))
length(uniquewords)
}
unique_word_count(text)
text_preprocessed <- Corpus(VectorSource(text)) %>%
tm_map(content_transformer(str_to_lower)) %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace)
unique_word_count(text_preprocessed)
myCorpus <- Corpus(VectorSource(text_preprocessed))
myStopwords <- c(stopwords('english'))
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
unique_word_count(text_preprocessed)
tdm <- TermDocumentMatrix(myCorpus)
m <- as.matrix(tdm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- tibble(word = names(v),freq=v)
d <- d %>% filter(word != tolower(search.string)) %>% arrange(desc(freq))
d <- d %>% filter(word != "the") %>% arrange(desc(freq))
d <- d %>% filter(word != "and") %>% arrange(desc(freq))
d <- d %>% filter(word != "with") %>% arrange(desc(freq))
d <- d %>% filter(word != "for") %>% arrange(desc(freq))
d <- d %>% filter(word != "you") %>% arrange(desc(freq))
d <- d %>% filter(word != "tesla") %>% arrange(desc(freq))
d <- d %>% filter(word != "elon") %>% arrange(desc(freq))
d <- d %>% filter(word != "musk") %>% arrange(desc(freq))
d <- d %>% filter(word != "elonmusk") %>% arrange(desc(freq))
options(warn=-1) #turn warnings off
wordcloud_tesla <- wordcloud2(d)
options(warn=0) #turn warnings back on
wordcloud_tesla <- wordcloud2(d)
# wordgraph
text <- iconv(text,'latin1', 'ascii', sub = '')
create_document_term_matrix <- function(data){
p_load(SnowballC, tm)
myCorpus <- Corpus(VectorSource(data))
cat("Transform to lower case \n")
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
cat("Remove punctuation \n")
myCorpus <- tm_map(myCorpus, removePunctuation)
cat("Remove numbers \n")
myCorpus <- tm_map(myCorpus, removeNumbers)
cat("Remove stopwords \n")
myStopwords <- c(stopwords('english'))
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
# cat("Stem corpus \n")
# myCorpus = tm_map(myCorpus, stemDocument);
# myCorpus = tm_map(myCorpus, stemCompletion, dictionary=dictCorpus);
cat("Create document by term matrix \n")
myDtm <- DocumentTermMatrix(myCorpus, control = list(wordLengths = c(2, Inf)))
myDtm
}
doc_term_mat <- create_document_term_matrix(text)
create_adjacency_matrix <- function(object, probs=0.99){
#object = output from function create_document_term_matrix (a document by term matrix)
#probs = select only vertexes with degree greater than or equal to quantile given by the value of probs
cat("Create adjacency matrix \n")
p_load(sna)
mat <- as.matrix(object)
mat[mat >= 1] <- 1 #change to boolean (adjacency) matrix
Z <- t(mat) %*% mat
cat("Apply filtering \n")
ind <- sna::degree(as.matrix(Z),cmode = "indegree") >= quantile(sna::degree(as.matrix(Z),cmode = "indegree"),probs=0.99)
#ind <- sna::betweenness(as.matrix(Z)) >= quantile(sna::betweenness(as.matrix(Z)),probs=0.99)
Z <- Z[ind,ind]
cat("Resulting adjacency matrix has ",ncol(Z)," rows and columns \n")
dim(Z)
list(Z=Z,termbydocmat=object,ind=ind)
}
adj_mat <- create_adjacency_matrix(doc_term_mat)
adj_mat[[1]][1:5,1:5]
plot_network <- function(object){
#Object: output from the create_adjacency_matrix function
#Create graph from adjacency matrix
p_load(igraph)
g <- graph.adjacency(object$Z, weighted=TRUE, mode ='undirected')
g <- simplify(g)
#Set labels and degrees of vertices
V(g)$label <- V(g)$name
V(g)$degree <- igraph::degree(g)
layout <- layout.auto(g)
opar <- par()$mar; par(mar=rep(0, 4)) #Give the graph lots of room
#Adjust the widths of the edges and add distance measure labels
#Use 1 - binary (?dist) a proportion distance of two vectors
#The binary distance (or Jaccard distance) measures the dissimilarity, so 1 is perfect and 0 is no overlap (using 1 - binary)
edge.weight <- 7  #A maximizing thickness constant
z1 <- edge.weight*(1-dist(t(object$termbydocmat)[object$ind,], method="binary"))
E(g)$width <- c(z1)[c(z1) != 0] #Remove 0s: these won't have an edge
clusters <- spinglass.community(g)
cat("Clusters found: ", length(clusters$csize),"\n")
cat("Modularity: ", clusters$modularity,"\n")
plot(g, layout=layout, vertex.color=rainbow(4)[clusters$membership], vertex.frame.color=rainbow(4)[clusters$membership] )
}
plot_network(adj_mat)
load("~/GitHub/SMWA_Performance/data/Scrape5_03Till13_03_Starbucks.RData")
tweets2
library(pacman)
p_load(twitteR)
tweets2
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\GitHub\\SMWA_Performance\\data")
load("First_Scrape9_03.Rdata")
tweets1 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape_09_03.Rdata")
tweets2 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_10_03.Rdata")
tweets3 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_13_03.Rdata")
tweets4 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_14_03.Rdata")
tweets5 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_15_03.Rdata")
tweets6 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_16_03.Rdata")
tweets7 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape2_16_03.Rdata")
tweets8 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape3_16_03.Rdata")
tweets9 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape4_16_03.Rdata")
tweets10 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape5_16_03.Rdata")
tweets11 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape5_03.Rdata")
tweets12 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape3_22_03.Rdata")
tweets13 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape3_19_03.Rdata")
tweets14 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape3_18_03.Rdata")
tweets15 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape2_22_03.Rdata")
tweets16 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape2_19_03.Rdata")
tweets17 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape2_18_03.Rdata")
tweets18 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
all_tweets <- rbind(tweets1, tweets2, tweets3, tweets4, tweets5, tweets6, tweets7, tweets8, tweets9, tweets10, tweets11)
colnames(all_tweets) <- c("user_id", "date", "text")
all_tweets <- rbind(tweets1, tweets2, tweets3, tweets4, tweets5, tweets6, tweets7, tweets8, tweets9, tweets10, tweets11, tweets12, tweets13, tweets14,
tweets15, tweets16, tweets17, tweets18)
View(tweets2)
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\GitHub\\SMWA_Performance\\data")
load("First_Scrape9_03.Rdata")
tweets1 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
#load("Scrape_09_03.Rdata")
#tweets2 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])   # there is something wrong with this file
load("Scrape1_10_03.Rdata")
tweets3 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_13_03.Rdata")
tweets4 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_14_03.Rdata")
tweets5 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_15_03.Rdata")
tweets6 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_16_03.Rdata")
tweets7 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape2_16_03.Rdata")
tweets8 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape3_16_03.Rdata")
tweets9 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape4_16_03.Rdata")
tweets10 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape5_16_03.Rdata")
tweets11 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape5_03.Rdata")
tweets12 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape3_22_03.Rdata")
tweets13 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape3_19_03.Rdata")
tweets14 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape3_18_03.Rdata")
tweets15 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape2_22_03.Rdata")
tweets16 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape2_19_03.Rdata")
tweets17 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape2_18_03.Rdata")
tweets18 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
all_tweets <- rbind(tweets1, tweets3, tweets4, tweets5, tweets6, tweets7, tweets8, tweets9, tweets10, tweets11, tweets12, tweets13, tweets14,
tweets15, tweets16, tweets17, tweets18)
colnames(all_tweets) <- c("user_id", "date", "text")
class(all_tweets)
sum(duplicated(all_tweets))
all_tweets <- all_tweets[!duplicated(all_tweets)]
all_tweets <- all_tweets[!duplicated(all_tweets), ]
save(all_tweets, file = "all_tweets.Rdata")
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\GitHub\\SMWA_Performance\\data")
rm(list=ls())
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\GitHub\\SMWA_Performance\\data")
load("First_Scrape9_03.Rdata")
tweets1 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
#load("Scrape_09_03.Rdata")
#tweets2 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])   # there is something wrong with this file
load("Scrape1_10_03.Rdata")
tweets3 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_13_03.Rdata")
tweets4 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_14_03.Rdata")
tweets5 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_15_03.Rdata")
tweets6 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_16_03.Rdata")
tweets7 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape2_16_03.Rdata")
tweets8 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape3_16_03.Rdata")
tweets9 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape4_16_03.Rdata")
tweets10 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape5_16_03.Rdata")
tweets11 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape5_03.Rdata")
tweets12 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape3_22_03.Rdata")
tweets13 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape3_19_03.Rdata")
tweets14 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape3_18_03.Rdata")
tweets15 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape2_22_03.Rdata")
tweets16 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape2_19_03.Rdata")
tweets17 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape2_18_03.Rdata")
tweets18 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_22_03.Rdata")
tweets19 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_19_03.Rdata")
tweets20 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape_17_03_tweets2.Rdata")
tweets21 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape_17_03.Rdata")
tweets22 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape_18_03.Rdata")
tweets23 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
all_tweets <- rbind(tweets1, tweets3, tweets4, tweets5, tweets6, tweets7, tweets8, tweets9, tweets10, tweets11, tweets12, tweets13, tweets14,
tweets15, tweets16, tweets17, tweets18, tweets19, tweets20, tweets21, tweets22, tweets23)
colnames(all_tweets) <- c("user_id", "date", "text")
class(all_tweets)
sum(duplicated(all_tweets))
all_tweets <- all_tweets[!duplicated(all_tweets), ]
save(all_tweets, file = "all_tweets.Rdata")
########################################
## DESCPRITIVE ##
rm(list=ls()) #Clean the environment
# Install and load packages
if (!require("pacman")) install.packages("pacman") ; require("pacman")
p_load(tidyverse, rtweet, httpuv, stringr, qdap, httr, wordcloud2, tm, tidytext, wordcloud)
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\SMWA")
source('tokensandkeys.R')
token <- get_token()
search.string <- "#Starbucks"
tweets <- search_tweets(search.string, n = 50000,
include_rts = FALSE,
retryonratelimit = FALSE,
lang = 'en',
token = token)
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\GitHub\\SMWA_Performance\\data")
save(tweets, file = "Scrape2_22_03.RData")
typeof(tweets)
tweets_table <- as.data.frame(tweets)
p_load(ggplot2)
plot(tweets$created_at)
hist(tweets$created_at)
tweets$retweet_location
plot(tweets_table$created_at)
text <- tweets_data(tweets) %>% pull(text)
myCorpus <- Corpus(VectorSource(text))
myCorpus <- Corpus(VectorSource(text)) %>%
tm_map(content_transformer(str_to_lower)) %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace)
myStopwords <- c(stopwords('english'))
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
tdm <- TermDocumentMatrix(myCorpus)
m <- as.matrix(tdm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- tibble(word = names(v),freq=v)
d <- d %>% filter(word != tolower(search.string)) %>% arrange(desc(freq))
wordcloud_starbucks <- wordcloud2(d)
wordcloud_starbucks
rm(list=ls())
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\GitHub\\SMWA_Performance\\data")
load("First_Scrape9_03.Rdata")
tweets1 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
#load("Scrape_09_03.Rdata")
#tweets2 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])   # there is something wrong with this file
load("Scrape1_10_03.Rdata")
tweets3 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_13_03.Rdata")
tweets4 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_14_03.Rdata")
tweets5 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_15_03.Rdata")
tweets6 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_16_03.Rdata")
tweets7 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape2_16_03.Rdata")
tweets8 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape3_16_03.Rdata")
tweets9 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape4_16_03.Rdata")
tweets10 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape5_16_03.Rdata")
tweets11 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape5_03.Rdata")
tweets12 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape3_22_03.Rdata")
tweets13 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape3_19_03.Rdata")
tweets14 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape3_18_03.Rdata")
tweets15 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape2_22_03.Rdata")
tweets16 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape2_19_03.Rdata")
tweets17 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape2_18_03.Rdata")
tweets18 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_22_03.Rdata")
tweets19 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_19_03.Rdata")
tweets20 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape_17_03_tweets2.Rdata")
tweets21 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape_17_03.Rdata")
tweets22 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape_18_03.Rdata")
tweets23 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape2_22_03.Rdata")
tweets24 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
all_tweets <- rbind(tweets1, tweets3, tweets4, tweets5, tweets6, tweets7, tweets8, tweets9, tweets10, tweets11, tweets12, tweets13, tweets14,
tweets15, tweets16, tweets17, tweets18, tweets19, tweets20, tweets21, tweets22, tweets23, tweets24)
colnames(all_tweets) <- c("user_id", "date", "text")
class(all_tweets)
sum(duplicated(all_tweets))
all_tweets <- all_tweets[!duplicated(all_tweets), ]
save(all_tweets, file = "all_tweets.Rdata")
View(all_tweets)
all_tweets <- format(as.POSIXct(all_tweets,format='%m/%d/%Y %H:%M:%S'),format='%m/%d/%Y')
rm(list=ls())
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\GitHub\\SMWA_Performance\\data")
load("First_Scrape9_03.Rdata")
tweets1 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
#load("Scrape_09_03.Rdata")
#tweets2 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])   # there is something wrong with this file
load("Scrape1_10_03.Rdata")
tweets3 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_13_03.Rdata")
tweets4 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_14_03.Rdata")
tweets5 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_15_03.Rdata")
tweets6 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_16_03.Rdata")
tweets7 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape2_16_03.Rdata")
tweets8 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape3_16_03.Rdata")
tweets9 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape4_16_03.Rdata")
tweets10 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape5_16_03.Rdata")
tweets11 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape5_03.Rdata")
tweets12 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape3_22_03.Rdata")
tweets13 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape3_19_03.Rdata")
tweets14 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape3_18_03.Rdata")
tweets15 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape2_22_03.Rdata")
tweets16 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape2_19_03.Rdata")
tweets17 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape2_18_03.Rdata")
tweets18 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_22_03.Rdata")
tweets19 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape1_19_03.Rdata")
tweets20 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape_17_03_tweets2.Rdata")
tweets21 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape_17_03.Rdata")
tweets22 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape_18_03.Rdata")
tweets23 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
load("Scrape2_22_03.Rdata")
tweets24 <- as.data.frame(tweets[, c("user_id", "created_at", "text")])
all_tweets <- rbind(tweets1, tweets3, tweets4, tweets5, tweets6, tweets7, tweets8, tweets9, tweets10, tweets11, tweets12, tweets13, tweets14,
tweets15, tweets16, tweets17, tweets18, tweets19, tweets20, tweets21, tweets22, tweets23, tweets24)
colnames(all_tweets) <- c("user_id", "date", "text")
all_tweets$date <- format(as.POSIXct(all_tweets$date,format='%m/%d/%Y %H:%M:%S'),format='%m/%d/%Y')
class(all_tweets)
sum(duplicated(all_tweets))
all_tweets <- all_tweets[!duplicated(all_tweets), ]
save(all_tweets, file = "all_tweets.Rdata")
rm(list=ls())
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\GitHub\\SMWA_Performance\\data")
load("First_Scrape9_03.Rdata")
rm(list=ls())
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\GitHub\\SMWA_Performance\\data")
load("Financial_Data.Rdata")
View(data)
load(all_tweets.Rdata)
########################################
## DESCPRITIVE ##
rm(list=ls()) #Clean the environment
# Install and load packages
if (!require("pacman")) install.packages("pacman") ; require("pacman")
p_load(tidyverse, rtweet, httpuv, stringr, qdap, httr, wordcloud2, tm, tidytext, wordcloud)
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\SMWA")
source('tokensandkeys.R')
token <- get_token()
search.string <- "#Starbucks"
tweets <- search_tweets(search.string, n = 50000,
include_rts = FALSE,
retryonratelimit = FALSE,
lang = 'en',
token = token)
text <- tweets_data(tweets) %>% pull(text)
myCorpus <- Corpus(VectorSource(text))
myCorpus <- Corpus(VectorSource(text)) %>%
tm_map(content_transformer(str_to_lower)) %>%
tm_map(removePunctuation) %>%
tm_map(removeNumbers) %>%
tm_map(stripWhitespace)
myStopwords <- c(stopwords('english'))
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
tdm <- TermDocumentMatrix(myCorpus)
m <- as.matrix(tdm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- tibble(word = names(v),freq=v)
d <- d %>% filter(word != tolower(search.string)) %>% arrange(desc(freq))
wordcloud_starbucks <- wordcloud2(d)
wordcloud_starbucks
rm(list=ls())
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\GitHub\\SMWA_Performance\\data")
load("Financial_Data.Rdata")
fin_data <- load("Financial_Data.Rdata")
fin_data
class(data)
rm(list=ls())
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\GitHub\\SMWA_Performance\\data")
load("Financial_Data.Rdata")
data$Target <- data$Target / data$Open
data <- subset(data, select = c(Date, Volume, Target))
rm(list=ls())
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\GitHub\\SMWA_Performance\\data")
load("Financial_Data.Rdata")
# set Target variable as a percentage
data$Target <- data$Target / data$Open
# remove columns that aren't necessary: Date is needed for merging, Volume and Close are independent variables, and Target is the dependent variable
data <- subset(data, select = c(Date, Volume, Close, Target))
# csv file with data on S&P 500 index
SP500 <- read.csv("S&P 500 Data.csv")
View(SP500)
SP500 <- subset(SP500, select = -c(Open, Hoog, Laag, Vol))
SP500 <- subset(SP500, select = -c(Open, Hoog, Laag, Vol.))
colnames(SP500) <- c(Date, SP500_close, SP500_change)
colnames(SP500) <- c("Date", "SP500_close", "SP500_change")
type(SP500$Date)
class(SP500$Date)
SP500 <- as.Date(SP500$Date, format =  "%d.%m.%Y")
rm(list=ls())
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\GitHub\\SMWA_Performance\\data")
load("Financial_Data.Rdata") # Wouter's csv needs to be updated to be recent enough
# set Target variable as a percentage
data$Target <- data$Target / data$Open
# remove columns that aren't necessary: Date is needed for merging, Volume and Close are independent variables, and Target is the dependent variable
data <- subset(data, select = c(Date, Volume, Close, Target))
# csv file with data on S&P 500 index
SP500 <- read.csv("S&P 500 Data.csv")
SP500 <- subset(SP500, select = -c(Open, Hoog, Laag, Vol.))
colnames(SP500) <- c("Date", "SP500_close", "SP500_change")
SP500$Date <- as.Date(SP500$Date, format =  "%d.%m.%Y")
data <- merge(data, SP500, by = "Date")
data$Date
SP500$Date
class(data$Date)
class(SP500$Date)
data$Date <- as.Date(data$Date, format = "%Y-%m-%d")
rm(list=ls())
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\GitHub\\SMWA_Performance\\data")
load("Financial_Data.Rdata") # Wouter's csv needs to be updated to be recent enough
# set Target variable as a percentage
data$Target <- data$Target / data$Open
# remove columns that aren't necessary: Date is needed for merging, Volume and Close are independent variables, and Target is the dependent variable
data <- subset(data, select = c(Date, Volume, Close, Target))
data$Date <- as.Date(data$Date, format = "%Y-%m-%d")
# csv file with data on S&P 500 index
SP500 <- read.csv("S&P 500 Data.csv")
SP500 <- subset(SP500, select = -c(Open, Hoog, Laag, Vol.))
colnames(SP500) <- c("Date", "SP500_close", "SP500_change")
SP500$Date <- as.Date(SP500$Date, format =  "%d.%m.%Y")
data <- merge(data, SP500, by = "Date")
