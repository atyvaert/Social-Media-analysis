unigrams <- str_split(text[i]," ")[[1]]
bigram <- rbind(unigrams, c(unigrams[2:(length(unigrams))],""))
bigrams <- paste(bigram[1, ], bigram[2, ])
#Find the positions of the words in the Tweet in the dictionary
m_uni <- match(unigrams, dictionary_neg$Word)
m_bi <- match(bigrams, dictionary_neg$Word)
#Which words are present in the dictionary?
present <- !(m_uni + m_bi == 0)
#Of the words that are present, select their valence
wordvalences_uni <- dictionary_neg$VALENCE[m_uni[present]]
wordvalences_bi <- dictionary_neg$VALENCE[m_bi[present]]
#Compute the mean valence of the tweet
scoretweet_uni[i] <- mean(wordvalences_uni, na.rm=TRUE)
scoretweet_bi[i] <- sum(wordvalences_uni)
#Handle the case when none of the words is in the dictionary
if (is.na(scoretweet_uni[i])) scoretweet_uni[i] <- 0 else scoretweet_uni[i] <- scoretweet_uni[i]
if (is.na(scoretweet_bi[i])) scoretweet_bi[i] <- 0 else scoretweet_bi[i] <- scoretweet_bi[i]
}
for (i in 1:length(text)){
#Transform text to lower case
text <- tolower(text)
#Split the tweet in words
unigrams <- str_split(text[i]," ")[[1]]
bigram <- rbind(unigrams, c(unigrams[2:(length(unigrams))],""))
bigrams <- paste(bigram[1, ], bigram[2, ])
#Find the positions of the words in the Tweet in the dictionary
m_uni <- match(unigrams, dictionary_neg$Word)
m_bi <- match(bigrams, dictionary_neg$Word)
#Which words are present in the dictionary?
present <- !(m_uni + m_bi == 0)
#Of the words that are present, select their valence
wordvalences_uni <- dictionary_neg$VALENCE[m_uni[present]]
wordvalences_bi <- dictionary_neg$VALENCE[m_bi[present]]
#Compute the mean valence of the tweet
scoretweet_uni[i] <- mean(wordvalences_uni, na.rm=TRUE)
scoretweet_bi[i] <- sum(wordvalences_bi)
#Handle the case when none of the words is in the dictionary
if (is.na(scoretweet_uni[i])) scoretweet_uni[i] <- 0 else scoretweet_uni[i] <- scoretweet_uni[i]
if (is.na(scoretweet_bi[i])) scoretweet_bi[i] <- 0 else scoretweet_bi[i] <- scoretweet_bi[i]
}
rm(list=ls()) #Clean the environment
# Install and load packages
if (!require("pacman")) install.packages("pacman") ; require("pacman")
p_load(tidyverse, rtweet, httpuv, stringr, qdap, httr, wordcloud2, tm, tidytext, wordcloud)
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\SMWA")
source('tokensandkeys.R')
token <- get_token()
search.string <- "#vettel"
tweets <- search_tweets(search.string, n = 110,
include_rts = FALSE,
retryonratelimit = FALSE,
lang = 'en',
token = token)
dictionary1 <- read_csv("dictionary.csv")
dictionary1 <- dictionary1 %>% mutate(across(where(is.numeric),function(x) x-5 ))
Word <- paste("not ", dictionary1$Word)
VALENCE <- -2 * dictionary1$VALENCE
AROUSAL <- NA
DOMINANCE <- NA
dictionary2 <- cbind(Word, VALENCE, AROUSAL, DOMINANCE)
dictionary_neg <- rbind(dictionary1, dictionary2)
text <- tweets_data(tweets) %>% pull(text)
Encoding(text) <- "latin1"
text <- iconv(text,'latin1', 'ascii', sub = '')
scoretweet_uni <- numeric(length(text))
scoretweet_bi <- numeric(length(text))
for (i in 1:length(text)){
#Transform text to lower case
text <- tolower(text)
#Split the tweet in words
unigrams <- str_split(text[i]," ")[[1]]
bigram <- rbind(unigrams, c(unigrams[2:(length(unigrams))],""))
bigrams <- paste(bigram[1, ], bigram[2, ])
#Find the positions of the words in the Tweet in the dictionary
m_uni <- match(unigrams, dictionary_neg$Word)
m_bi <- match(bigrams, dictionary_neg$Word)
#Which words are present in the dictionary?
present <- !(m_uni + m_bi == 0)
#Of the words that are present, select their valence
wordvalences_uni <- dictionary_neg$VALENCE[m_uni[present]]
wordvalences_bi <- dictionary_neg$VALENCE[m_bi[present]]
#Compute the mean valence of the tweet
scoretweet_uni[i] <- mean(wordvalences_uni, na.rm=TRUE)
scoretweet_bi[i] <- mean(wordvalences_bi, na.rm=TRUE)
scoretweet_bi[i] <- scoretweet_uni[i] + scoretweet_bi[i]
#Handle the case when none of the words is in the dictionary
if (is.na(scoretweet_uni[i])) scoretweet_uni[i] <- 0 else scoretweet_uni[i] <- scoretweet_uni[i]
if (is.na(scoretweet_bi[i])) scoretweet_bi[i] <- 0 else scoretweet_bi[i] <- scoretweet_bi[i]
}
hist(scoretweet_uni)
View(bind_cols(scoretweet_uni,text))
for (i in 1:length(text)){
#Transform text to lower case
text <- tolower(text)
#Split the tweet in words
unigrams <- str_split(text[i]," ")[[1]]
bigram <- rbind(unigrams, c(unigrams[2:(length(unigrams))],""))
bigrams <- paste(bigram[1, ], bigram[2, ])
#Find the positions of the words in the Tweet in the dictionary
m_uni <- match(unigrams, dictionary_neg$Word)
m_bi <- match(bigrams, dictionary_neg$Word)
#Which words are present in the dictionary?
present <- !(m_uni + m_bi == 0)
#Of the words that are present, select their valence
wordvalences_uni <- dictionary_neg$VALENCE[m_uni[present]]
wordvalences_bi <- dictionary_neg$VALENCE[m_bi[present]]
#Compute the mean valence of the tweet
scoretweet_uni[i] <- mean(wordvalences_uni, na.rm=TRUE)
scoretweet_bi[i] <- mean(wordvalences_bi, na.rm=TRUE)
#Handle the case when none of the words is in the dictionary
if (is.na(scoretweet_uni[i])) scoretweet_uni[i] <- 0 else scoretweet_uni[i] <- scoretweet_uni[i]
if (is.na(scoretweet_bi[i])) scoretweet_bi[i] <- 0 else scoretweet_bi[i] <- scoretweet_bi[i]
}
hist(scoretweet_uni)
scoretweet_bi
bigram
bigrams
m_bi
length(m_bi)
Word <- paste("no ", dictionary1$Word)
VALENCE <- -2 * dictionary1$VALENCE
AROUSAL <- NA
DOMINANCE <- NA
dictionary2 <- cbind(Word, VALENCE, AROUSAL, DOMINANCE)
dictionary_neg <- rbind(dictionary1, dictionary2)
text <- tweets_data(tweets) %>% pull(text)
Encoding(text) <- "latin1"
text <- iconv(text,'latin1', 'ascii', sub = '')
scoretweet_uni <- numeric(length(text))
scoretweet_bi <- numeric(length(text))
for (i in 1:length(text)){
#Transform text to lower case
text <- tolower(text)
#Split the tweet in words
unigrams <- str_split(text[i]," ")[[1]]
bigram <- rbind(unigrams, c(unigrams[2:(length(unigrams))],""))
bigrams <- paste(bigram[1, ], bigram[2, ])
#Find the positions of the words in the Tweet in the dictionary
m_uni <- match(unigrams, dictionary_neg$Word)
m_bi <- match(bigrams, dictionary_neg$Word)
#Which words are present in the dictionary?
present <- !(m_uni + m_bi == 0)
#Of the words that are present, select their valence
wordvalences_uni <- dictionary_neg$VALENCE[m_uni[present]]
wordvalences_bi <- dictionary_neg$VALENCE[m_bi[present]]
#Compute the mean valence of the tweet
scoretweet_uni[i] <- mean(wordvalences_uni, na.rm=TRUE)
scoretweet_bi[i] <- mean(wordvalences_bi, na.rm=TRUE)
#Handle the case when none of the words is in the dictionary
if (is.na(scoretweet_uni[i])) scoretweet_uni[i] <- 0 else scoretweet_uni[i] <- scoretweet_uni[i]
if (is.na(scoretweet_bi[i])) scoretweet_bi[i] <- 0 else scoretweet_bi[i] <- scoretweet_bi[i]
}
hist(scoretweet_uni)
View(bind_cols(scoretweet_uni,text))
for (i in 1:length(text)){
#Transform text to lower case
text <- tolower(text)
#Split the tweet in words
unigrams <- str_split(text[i]," ")[[1]]
bigram <- rbind(unigrams, c(unigrams[2:(length(unigrams))],""))
bigrams <- paste(bigram[1, ], bigram[2, ])
#Find the positions of the words in the Tweet in the dictionary
m_uni <- match(unigrams, dictionary_neg$Word)
m_bi <- match(bigrams, dictionary_neg$Word)
#Which words are present in the dictionary?
present <- !(m_uni + m_bi == 0)
#Of the words that are present, select their valence
wordvalences_uni <- dictionary_neg$VALENCE[m_uni[present]]
wordvalences_bi <- dictionary_neg$VALENCE[m_bi[present]]
#Compute the mean valence of the tweet
scoretweet_uni[i] <- mean(wordvalences_uni, na.rm=TRUE)
scoretweet_bi[i] <- mean(wordvalences_bi, na.rm=TRUE)
#Handle the case when none of the words is in the dictionary
if (is.na(scoretweet_uni[i])) scoretweet_uni[i] <- 0 else scoretweet_uni[i] <- scoretweet_uni[i]
if (is.na(scoretweet_bi[i])) scoretweet_bi[i] <- 0 else scoretweet_bi[i] <- scoretweet_bi[i]
}
hist(scoretweet_uni)
View(bind_cols(scoretweet_uni,text))
for (i in 1:length(text)){
#Transform text to lower case
text <- tolower(text)
#Split the tweet in words
unigrams <- str_split(text[i]," ")[[1]]
bigram <- rbind(unigrams, c(unigrams[2:(length(unigrams))],""))
bigrams <- paste(bigram[1, ], bigram[2, ])
#Find the positions of the words in the Tweet in the dictionary
m_uni <- match(unigrams, dictionary_neg$Word)
m_bi <- match(bigrams, dictionary_neg$Word)
#Which words are present in the dictionary?
present <- (is.na(m_uni) | is.na(m_bi))
#Of the words that are present, select their valence
wordvalences_uni <- dictionary_neg$VALENCE[m_uni[present]]
wordvalences_bi <- dictionary_neg$VALENCE[m_bi[present]]
#Compute the mean valence of the tweet
scoretweet_uni[i] <- mean(wordvalences_uni, na.rm=TRUE)
scoretweet_bi[i] <- mean(wordvalences_bi, na.rm=TRUE)
#Handle the case when none of the words is in the dictionary
if (is.na(scoretweet_uni[i])) scoretweet_uni[i] <- 0 else scoretweet_uni[i] <- scoretweet_uni[i]
if (is.na(scoretweet_bi[i])) scoretweet_bi[i] <- 0 else scoretweet_bi[i] <- scoretweet_bi[i]
}
hist(scoretweet_uni)
View(bind_cols(scoretweet_uni,text))
rm(list=ls()) #Clean the environment
# Install and load packages
if (!require("pacman")) install.packages("pacman") ; require("pacman")
p_load(tidyverse, rtweet, httpuv, stringr, qdap, httr, wordcloud2, tm, tidytext, wordcloud)
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\SMWA")
source('tokensandkeys.R')
token <- get_token()
search.string <- "#vettel"
tweets <- search_tweets(search.string, n = 110,
include_rts = FALSE,
retryonratelimit = FALSE,
lang = 'en',
token = token)
dictionary1 <- read_csv("dictionary.csv")
dictionary1 <- dictionary1 %>% mutate(across(where(is.numeric),function(x) x-5 ))
Word <- paste("no ", dictionary1$Word)
VALENCE <- -2 * dictionary1$VALENCE
AROUSAL <- NA
DOMINANCE <- NA
dictionary2 <- cbind(Word, VALENCE, AROUSAL, DOMINANCE)
dictionary_neg <- rbind(dictionary1, dictionary2)
text <- tweets_data(tweets) %>% pull(text)
Encoding(text) <- "latin1"
text <- iconv(text,'latin1', 'ascii', sub = '')
scoretweet_uni <- numeric(length(text))
scoretweet_bi <- numeric(length(text))
for (i in 1:length(text)){
#Transform text to lower case
text <- tolower(text)
#Split the tweet in words
unigrams <- str_split(text[i]," ")[[1]]
bigram <- rbind(unigrams, c(unigrams[2:(length(unigrams))],""))
bigrams <- paste(bigram[1, ], bigram[2, ])
#Find the positions of the words in the Tweet in the dictionary
m_uni <- match(unigrams, dictionary_neg$Word)
m_bi <- match(bigrams, dictionary_neg$Word)
#Which words are present in the dictionary?
present <- (is.na(m_uni) | is.na(m_bi))
#Of the words that are present, select their valence
wordvalences_uni <- dictionary_neg$VALENCE[m_uni[present]]
wordvalences_bi <- dictionary_neg$VALENCE[m_bi[present]]
#Compute the mean valence of the tweet
scoretweet_uni[i] <- mean(wordvalences_uni, na.rm=TRUE)
scoretweet_bi[i] <- mean(wordvalences_bi, na.rm=TRUE)
#Handle the case when none of the words is in the dictionary
if (is.na(scoretweet_uni[i])) scoretweet_uni[i] <- 0 else scoretweet_uni[i] <- scoretweet_uni[i]
if (is.na(scoretweet_bi[i])) scoretweet_bi[i] <- 0 else scoretweet_bi[i] <- scoretweet_bi[i]
}
hist(scoretweet_uni)
hist(scoretweet_bi)
View(bind_cols(scoretweet_bi,text))
created <- tweets_data(tweets) %>% pull(created_at)
p_load(lubridate)
time <- ymd_hms(created, format="%Y-%m-%d %H:%M:%S",tz="UTC")
attributes(time)$tzone <- "CET"
time <- na.omit(time)
breakshour <- hour(time)
breaksmin <- minute(time)
scores <- tibble(scoretweet_bi, hourminute =  paste(breakshour, breaksmin, sep = ":"))
scores <- scores %>% group_by(hourminute) %>% summarise(sentiment=mean(scoretweet))
lim <- max(abs(scores$sentiment))
plot(1:length(scores$sentiment),
rev(scores$sentiment),
xaxt="n",
type="l",
ylab="Valence",
xlab="Time (hour:minute)",
main="Sentiment",
ylim=c(-lim,lim))
scores <- tibble(scoretweet_bi, hourminute =  paste(breakshour, breaksmin, sep = ":"))
scores <- scores %>% group_by(hourminute) %>% summarise(sentiment=mean(scoretweet_bi))
lim <- max(abs(scores$sentiment))
plot(1:length(scores$sentiment),
rev(scores$sentiment),
xaxt="n",
type="l",
ylab="Valence",
xlab="Time (hour:minute)",
main="Sentiment",
ylim=c(-lim,lim))
bigrams
i=130
#Split the tweet in words
unigrams <- str_split(text[i]," ")[[1]]
bigram <- rbind(unigrams, c(unigrams[2:(length(unigrams))],""))
bigrams <- paste(bigram[1, ], bigram[2, ])
#Find the positions of the words in the Tweet in the dictionary
m_uni <- match(unigrams, dictionary_neg$Word)
m_bi <- match(bigrams, dictionary_neg$Word)
bigrams
m_bi
Word <- paste("\nno ", dictionary1$Word)
VALENCE <- -2 * dictionary1$VALENCE
AROUSAL <- NA
DOMINANCE <- NA
dictionary2 <- cbind(Word, VALENCE, AROUSAL, DOMINANCE)
dictionary_neg <- rbind(dictionary1, dictionary2)
text <- tweets_data(tweets) %>% pull(text)
Encoding(text) <- "latin1"
text <- iconv(text,'latin1', 'ascii', sub = '')
m_bi
bigrams
m_bi
Word <- paste("\n\nno ", dictionary1$Word)
VALENCE <- -2 * dictionary1$VALENCE
AROUSAL <- NA
DOMINANCE <- NA
dictionary2 <- cbind(Word, VALENCE, AROUSAL, DOMINANCE)
dictionary_neg <- rbind(dictionary1, dictionary2)
text <- tweets_data(tweets) %>% pull(text)
Encoding(text) <- "latin1"
text <- iconv(text,'latin1', 'ascii', sub = '')
bigrams
m_bi
#Transform text to lower case
text <- tolower(text)
#Split the tweet in words
unigrams <- str_split(text[i]," ")[[1]]
bigram <- rbind(unigrams, c(unigrams[2:(length(unigrams))],""))
bigrams <- paste(bigram[1, ], bigram[2, ])
#Find the positions of the words in the Tweet in the dictionary
m_uni <- match(unigrams, dictionary_neg$Word)
m_bi <- match(bigrams, dictionary_neg$Word)
m_bi
dictionary_neg$Word
dictionary_neg[20000, 1]
rm(list=ls()) #Clean the environment
# Install and load packages
if (!require("pacman")) install.packages("pacman") ; require("pacman")
p_load(tidyverse, rtweet, httpuv, stringr, qdap, httr, wordcloud2, tm, tidytext, wordcloud)
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\SMWA")
source('tokensandkeys.R')
token <- get_token()
search.string <- "#vettel"
tweets <- search_tweets(search.string, n = 110,
include_rts = FALSE,
retryonratelimit = FALSE,
lang = 'en',
token = token)
dictionary1 <- read_csv("dictionary.csv")
dictionary1 <- dictionary1 %>% mutate(across(where(is.numeric),function(x) x-5 ))
Word <- paste("no ", dictionary1$Word)
VALENCE <- -2 * dictionary1$VALENCE
AROUSAL <- NA
DOMINANCE <- NA
dictionary2 <- cbind(Word, VALENCE, AROUSAL, DOMINANCE)
dictionary_neg <- rbind(dictionary1, dictionary2)
text <- tweets_data(tweets) %>% pull(text)
Encoding(text) <- "latin1"
text <- iconv(text,'latin1', 'ascii', sub = '')
scoretweet_uni <- numeric(length(text))
scoretweet_bi <- numeric(length(text))
for (i in 1:length(text)){
#Transform text to lower case
text <- tolower(text)
#Split the tweet in words
unigrams <- str_split(text[i]," ")[[1]]
bigram <- rbind(unigrams, c(unigrams[2:(length(unigrams))],""))
bigrams <- paste(bigram[1, ], bigram[2, ])
#Find the positions of the words in the Tweet in the dictionary
m_uni <- match(unigrams, dictionary_neg$Word)
m_bi <- match(bigrams, dictionary_neg$Word)
#Which words are present in the dictionary?
present <- (is.na(m_uni) | is.na(m_bi))
#Of the words that are present, select their valence
wordvalences_uni <- dictionary_neg$VALENCE[m_uni[present]]
wordvalences_bi <- dictionary_neg$VALENCE[m_bi[present]]
#Compute the mean valence of the tweet
scoretweet_uni[i] <- mean(wordvalences_uni, na.rm=TRUE)
scoretweet_bi[i] <- mean(wordvalences_bi, na.rm=TRUE)
#Handle the case when none of the words is in the dictionary
if (is.na(scoretweet_uni[i])) scoretweet_uni[i] <- 0 else scoretweet_uni[i] <- scoretweet_uni[i]
if (is.na(scoretweet_bi[i])) scoretweet_bi[i] <- 0 else scoretweet_bi[i] <- scoretweet_bi[i]
}
hist(scoretweet_bi)
for (i in 1:length(text)){
#Transform text to lower case
text <- tolower(text)
#Split the tweet in words
unigrams <- str_split(text[i]," ")[[1]]
bigram <- rbind(unigrams, c(unigrams[2:(length(unigrams))],""))
bigrams <- paste(bigram[1, ], bigram[2, ])
#Find the positions of the words in the Tweet in the dictionary
m_uni <- match(unigrams, dictionary_neg$Word)
m_bi <- match(bigrams, dictionary_neg$Word)
#Which words are present in the dictionary?
present <- (is.na(m_uni) | is.na(m_bi))
#Of the words that are present, select their valence
wordvalences_uni <- dictionary_neg$VALENCE[m_uni[present]]
wordvalences_bi <- dictionary_neg$VALENCE[m_bi[present]] + wordvalences_uni
#Compute the mean valence of the tweet
scoretweet_uni[i] <- mean(wordvalences_uni, na.rm=TRUE)
scoretweet_bi[i] <- mean(wordvalences_bi, na.rm=TRUE)
#Handle the case when none of the words is in the dictionary
if (is.na(scoretweet_uni[i])) scoretweet_uni[i] <- 0 else scoretweet_uni[i] <- scoretweet_uni[i]
if (is.na(scoretweet_bi[i])) scoretweet_bi[i] <- 0 else scoretweet_bi[i] <- scoretweet_bi[i]
}
dictionary_neg$VALENCE[m_uni[present]]
########################################
## DESCPRITIVE ##
rm(list=ls()) #Clean the environment
# Install and load packages
if (!require("pacman")) install.packages("pacman") ; require("pacman")
p_load(tidyverse, rtweet, httpuv, stringr, qdap, httr, wordcloud2, tm, tidytext, wordcloud)
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\SMWA")
source('tokensandkeys.R')
token <- get_token()
search.string <- "#Starbucks"
tweets <- search_tweets(search.string, n = 18000,
include_rts = FALSE,
retryonratelimit = FALSE,
lang = 'en',
token = get_token())
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\GitHub\\SMWA_Performance\\data")
save(tweets, file = "Scrape1_15_03.RData")
tweets$text
rm(list=ls()) #Clean the environment
# Install and load packages
if (!require("pacman")) install.packages("pacman") ; require("pacman")
p_load(tidyverse, rtweet, httpuv, stringr, qdap, httr, wordcloud2, tm, tidytext, wordcloud)
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\SMWA")
source('tokensandkeys.R')
token <- get_token()
search.string <- "#vettel"
tweets <- search_tweets(search.string, n = 110,
include_rts = FALSE,
retryonratelimit = FALSE,
lang = 'en',
token = token)
dictionary1 <- read_csv("dictionary.csv")
dictionary1 <- dictionary1 %>% mutate(across(where(is.numeric),function(x) x-5 ))
Word <- paste("no ", dictionary1$Word)
VALENCE <- -2 * dictionary1$VALENCE
AROUSAL <- NA
DOMINANCE <- NA
dictionary2 <- cbind(Word, VALENCE, AROUSAL, DOMINANCE)
dictionary_neg <- rbind(dictionary1, dictionary2)
text <- tweets_data(tweets) %>% pull(text)
Encoding(text) <- "latin1"
text <- iconv(text,'latin1', 'ascii', sub = '')
scoretweet_uni <- numeric(length(text))
scoretweet_bi <- numeric(length(text))
for (i in 1:length(text)){
#Transform text to lower case
text <- tolower(text)
#Split the tweet in words
unigrams <- str_split(text[i]," ")[[1]]
bigram <- rbind(unigrams, c(unigrams[2:(length(unigrams))],""))
bigrams <- paste(bigram[1, ], bigram[2, ])
#Find the positions of the words in the Tweet in the dictionary
m_uni <- match(unigrams, dictionary_neg$Word)
m_bi <- match(bigrams, dictionary_neg$Word)
#Which words are present in the dictionary?
present <- (is.na(m_uni) | is.na(m_bi))
#Of the words that are present, select their valence
wordvalences_uni <- dictionary_neg$VALENCE[m_uni[present]]
wordvalences_bi <- dictionary_neg$VALENCE[m_bi[present]] + wordvalences_uni
#Compute the mean valence of the tweet
scoretweet_uni[i] <- mean(wordvalences_uni, na.rm=TRUE)
scoretweet_bi[i] <- mean(wordvalences_bi, na.rm=TRUE)
#Handle the case when none of the words is in the dictionary
if (is.na(scoretweet_uni[i])) scoretweet_uni[i] <- 0 else scoretweet_uni[i] <- scoretweet_uni[i]
if (is.na(scoretweet_bi[i])) scoretweet_bi[i] <- 0 else scoretweet_bi[i] <- scoretweet_bi[i]
}
hist(scoretweet_bi)
sum(scoretweet_bi)
########################################
## DESCPRITIVE ##
rm(list=ls()) #Clean the environment
# Install and load packages
if (!require("pacman")) install.packages("pacman") ; require("pacman")
p_load(tidyverse, rtweet, httpuv, stringr, qdap, httr, wordcloud2, tm, tidytext, wordcloud)
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\SMWA")
source('tokensandkeys.R')
token <- get_token()
search.string <- "#Starbucks"
tweets <- search_tweets(search.string, n = 50000,
include_rts = FALSE,
retryonratelimit = FALSE,
lang = 'en',
token = token)
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\GitHub\\SMWA_Performance\\data")
save(tweets, file = "Scrape2_16_03.RData")
tweets <- search_tweets(search.string, n = 50000,
include_rts = FALSE,
retryonratelimit = FALSE,
lang = 'en',
token = token)
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\GitHub\\SMWA_Performance\\data")
save(tweets, file = "Scrape3_16_03.RData")
tweets <- search_tweets(search.string, n = 50000,
include_rts = FALSE,
retryonratelimit = FALSE,
lang = 'en',
token = token)
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\GitHub\\SMWA_Performance\\data")
save(tweets, file = "Scrape4_16_03.RData")
tweets <- search_tweets(search.string, n = 50000,
include_rts = FALSE,
retryonratelimit = FALSE,
lang = 'en',
token = token)
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\GitHub\\SMWA_Performance\\data")
save(tweets, file = "Scrape5_16_03.RData")
tweets <- search_tweets(search.string, n = 50000,
include_rts = FALSE,
retryonratelimit = FALSE,
lang = 'en',
token = token)
setwd("C:\\Users\\bertj\\OneDrive\\Documenten\\GitHub\\SMWA_Performance\\data")
